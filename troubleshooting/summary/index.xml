<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>踩坑总结 on Kubernetes 实践指南</title>
    <link>https://k8s.imroc.io/troubleshooting/summary/</link>
    <description>Recent content in 踩坑总结 on Kubernetes 实践指南</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    
	<atom:link href="https://k8s.imroc.io/troubleshooting/summary/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>cgroup 泄露</title>
      <link>https://k8s.imroc.io/troubleshooting/summary/cgroup-leaking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/summary/cgroup-leaking/</guid>
      <description>内核 Bug memcg 是 Linux 内核中用于管理 cgroup 内存的模块，整个生命周期应该是跟随 cgroup 的，但是在低版本内核中(已知3.10)，一旦给某个 memory cgroup 开启 kmem accounting 中的 memory.kmem.limit_in_bytes 就可能会导致不能彻底删除 memcg 和对应的 cssid，也就是说应用即使已经删除了 cgroup (/sys/fs/cgroup/memory 下对应的 cgroup 目录已经删除), 但在内核中没有释放 cssid，导致内核认为的 cgroup 的数量实际数量不一致，我们也无法得知内核认为的 cgroup 数量是多少。
关于 cgroup kernel memory，在 kernel.org 中有如下描述：
2.7 Kernel Memory Extension (CONFIG_MEMCG_KMEM) ----------------------------------------------- With the Kernel memory extension, the Memory Controller is able to limit the amount of kernel memory used by the system. Kernel memory is fundamentally different than user memory, since it can&#39;t be swapped out, which makes it possible to DoS the system by consuming too much of this precious resource.</description>
    </item>
    
    <item>
      <title>conntrack 冲突导致丢包</title>
      <link>https://k8s.imroc.io/troubleshooting/summary/conntrack-conflict/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/summary/conntrack-conflict/</guid>
      <description>TODO</description>
    </item>
    
    <item>
      <title>tcp tw recycle 引发丢包</title>
      <link>https://k8s.imroc.io/troubleshooting/summary/tcp_tw_recycle-causes-packet-loss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/summary/tcp_tw_recycle-causes-packet-loss/</guid>
      <description>tcp_tw_recycle 这个内核参数用来快速回收 TIME_WAIT 连接，不过如果在 NAT 环境下会引发问题。
RFC1323 中有如下一段描述：
An additional mechanism could be added to the TCP, a per-host cache of the last timestamp received from any connection. This value could then be used in the PAWS mechanism to reject old duplicate segments from earlier incarnations of the connection, if the timestamp clock can be guaranteed to have ticked at least once since the old connection was open. This would require that the TIME-WAIT delay plus the RTT together must be at least one tick of the sender’s timestamp clock.</description>
    </item>
    
    <item>
      <title>使用 oom-guard 在用户态处理 cgroup OOM</title>
      <link>https://k8s.imroc.io/troubleshooting/summary/handle-cgroup-oom-in-userspace-with-oom-guard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/summary/handle-cgroup-oom-in-userspace-with-oom-guard/</guid>
      <description>背景 由于 linux 内核对 cgroup OOM 的处理，存在很多 bug，经常有由于频繁 cgroup OOM 导致节点故障(卡死， 重启， 进程异常但无法杀死)，于是 TKE 团队开发了 oom-guard，在用户态处理 cgroup OOM 规避了内核 bug。
原理 核心思想是在发生内核 cgroup OOM kill 之前，在用户空间杀掉超限的容器， 减少走到内核 cgroup 内存回收失败后的代码分支从而触发各种内核故障的机会。
threshold notify 参考文档: https://lwn.net/Articles/529927/
oom-guard 会给 memory cgroup 设置 threshold notify， 接受内核的通知。
以一个例子来说明阀值计算通知原理: 一个 pod 设置的 memory limit 是 1000M， oom-guard 会根据配置参数计算出 margin:
margin = 1000M * margin_ratio = 20M // 缺省margin_ratio是0.02 margin 最小不小于 mim_margin(缺省1M)， 最大不大于 max_margin(缺省为30M)。如果超出范围，则取 mim_margin 或 max_margin。计算 threshold = limit - margin ，也就是 1000M - 20M = 980M，把 980M 作为阈值设置给内核。当这个 pod 的内存使用量达到 980M 时， oom-guard 会收到内核的通知。</description>
    </item>
    
  </channel>
</rss>