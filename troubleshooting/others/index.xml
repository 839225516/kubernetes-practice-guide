<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>其它排错 on Kubernetes 实践指南</title>
    <link>https://k8s.imroc.io/troubleshooting/others/</link>
    <description>Recent content in 其它排错 on Kubernetes 实践指南</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    
	<atom:link href="https://k8s.imroc.io/troubleshooting/others/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>APIServer 响应慢</title>
      <link>https://k8s.imroc.io/troubleshooting/others/slow-apiserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/others/slow-apiserver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Daemonset 没有被调度</title>
      <link>https://k8s.imroc.io/troubleshooting/others/daemonset-not-scheduled/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/others/daemonset-not-scheduled/</guid>
      <description>Daemonset 的期望实例为 0，可能原因:
 controller-manager 的 bug，重启 controller-manager 可以恢复 controller-manager 挂了  </description>
    </item>
    
    <item>
      <title>Job 无法被删除</title>
      <link>https://k8s.imroc.io/troubleshooting/others/job-cannot-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/others/job-cannot-delete/</guid>
      <description>原因  可能是 k8s 的一个bug: https://github.com/kubernetes/kubernetes/issues/43168 本质上是脏数据问题，Running+Succeed != 期望Completions 数量，低版本 kubectl 不容忍，delete job 的时候打开debug(加-v=8)，会看到kubectl不断在重试，直到达到timeout时间。新版kubectl会容忍这些，删除job时会删除关联的pod  解决方法  升级 kubectl 版本，1.12 以上 低版本 kubectl 删除 job 时带 --cascade=false 参数(如果job关联的pod没删完，加这个参数不会删除关联的pod)  kubectl delete job --cascade=false &amp;lt;job name&amp;gt; </description>
    </item>
    
    <item>
      <title>kubectl 执行 exec 或 logs 失败</title>
      <link>https://k8s.imroc.io/troubleshooting/others/kubectl-exec-or-logs-failed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/others/kubectl-exec-or-logs-failed/</guid>
      <description>通常是 apiserver &amp;ndash;&amp;gt; kubelet:10250 之间的网络不通，10250 是 kubelet 提供接口的端口，kubectl exec 和 kubectl logs 的原理就是 apiserver 调 kubelet，kubelet 再调运行时 (比如 dockerd) 来实现的，所以要保证 kubelet 10250 端口对 apiserver 放通。检查防火墙、iptables 规则是否对 10250 端口或某些 IP 进行了拦截。</description>
    </item>
    
    <item>
      <title>Namespace 卡在 Terminating</title>
      <link>https://k8s.imroc.io/troubleshooting/others/namespace-stuck-on-terminating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/others/namespace-stuck-on-terminating/</guid>
      <description>Namespace 上存在 Finalizers 删除 ns 后，一直卡在 Terminating 状态。通常是存在 finalizers，通过 kubectl get ns xxx -o yaml 可以看到是否有 finalizers:
$ kubectl get ns -o yaml kube-node-lease apiVersion: v1 kind: Namespace metadata: ... finalizers: - finalizers.kubesphere.io/namespaces labels: kubesphere.io/workspace: system-workspace name: kube-node-lease ownerReferences: - apiVersion: tenant.kubesphere.io/v1alpha1 blockOwnerDeletion: true controller: true kind: Workspace name: system-workspace uid: d4310acd-1fdc-11ea-a370-a2c490b9ae47 spec: {} 此例是因为之前装过 kubesphere，然后卸载了，但没有清理 finalizers，将其删除就可以了。
k8s 资源的 metadata 里如果存在 finalizers，那么该资源一般是由某应用创建的，或者是这个资源是此应用关心的。应用会在资源的 metadata 里的 finalizers 加了一个它自己可以识别的标识，这意味着这个资源被删除时需要由此应用来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。</description>
    </item>
    
    <item>
      <title>Node 全部消失</title>
      <link>https://k8s.imroc.io/troubleshooting/others/node-all-gone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/others/node-all-gone/</guid>
      <description>Rancher 清除 Node 导致集群异常 现象 安装了 rancher 的用户，在卸载 rancher 的时候，可能会手动执行 kubectl delete ns local 来删除这个 rancher 创建的 namespace，但直接这样做会导致所有 node 被清除，通过 kubectl get node 获取不到 node。
原因 看了下 rancher 源码，rancher 通过 nodes.management.cattle.io 这个 CRD 存储和管理 node，会给所有 node 创建对应的这个 CRD 资源，metadata 中加入了两个 finalizer，其中 user-node-remove_local 对应的 finalizer 处理逻辑就是删除对应的 k8s node 资源，也就是 delete ns local 时，会尝试删除 nodes.management.cattle.io 这些 CRD 资源，进而触发 rancher 的 finalizer 逻辑去删除对应的 k8s node 资源，从而清空了 node，所以 kubectl get node 就看不到 node 了，集群里的服务就无法被调度。</description>
    </item>
    
  </channel>
</rss>