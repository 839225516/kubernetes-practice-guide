<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>节点排错 on Kubernetes 实践指南</title>
    <link>https://k8s.imroc.io/troubleshooting/node/</link>
    <description>Recent content in 节点排错 on Kubernetes 实践指南</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    
	<atom:link href="https://k8s.imroc.io/troubleshooting/node/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>arp_cache: neighbor table overflow! (arp缓存溢出)</title>
      <link>https://k8s.imroc.io/troubleshooting/node/arp_cache-neighbor-table-overflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/node/arp_cache-neighbor-table-overflow/</guid>
      <description>节点内核报这个错说明当前节点 arp 缓存满了。
查看当前 arp 记录数:
$ arp -an | wc -l 1335 查看 gc 阀值:
$ sysctl -a | grep net.ipv4.neigh.default.gc_thresh net.ipv4.neigh.default.gc_thresh1 = 128 net.ipv4.neigh.default.gc_thresh2 = 512 net.ipv4.neigh.default.gc_thresh3 = 1024 当前 arp 记录数接近 gc_thresh3 比较容易 overflow，因为当 arp 记录达到 gc_thresh3 时会强制触发 gc 清理，当这时又有数据包要发送，并且根据目的 IP 在 arp cache 中没找到 mac 地址，这时会判断当前 arp cache 记录数加 1 是否大于 gc_thresh3，如果没有大于就会 时就会报错: neighbor table overflow!
什么场景下会发生 集群规模大，node 和 pod 数量超多，参考本书避坑宝典的 案例分享: ARP 缓存爆满导致健康检查失败
解决方案 调整部分节点内核参数，将 arp cache 的 gc 阀值调高 (/etc/sysctl.</description>
    </item>
    
    <item>
      <title>Cannot allocate memory</title>
      <link>https://k8s.imroc.io/troubleshooting/node/cannot-allocate-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/node/cannot-allocate-memory/</guid>
      <description>容器启动失败，报错 Cannot allocate memory。
PID 耗尽 如果登录 ssh 困难，并且登录成功后执行任意命名经常报 Cannot allocate memory，多半是 PID 耗尽了。
处理方法参考本书 处理实践: PID 耗尽</description>
    </item>
    
    <item>
      <title>no space left on device</title>
      <link>https://k8s.imroc.io/troubleshooting/node/no-space-left-on-device/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/node/no-space-left-on-device/</guid>
      <description>有时候节点 NotReady， kubelet 日志报 no space left on device 有时候创建 Pod 失败，describe pod 看 event 报 no space left on device  出现这种错误有很多中可能原因，下面我们来根据现象找对应原因。
inotify watch 耗尽 节点 NotReady，kubelet 启动失败，看 kubelet 日志:
Jul 18 15:20:58 VM_16_16_centos kubelet[11519]: E0718 15:20:58.280275 11519 raw.go:140] Failed to watch directory &amp;#34;/sys/fs/cgroup/memory/kubepods&amp;#34;: inotify_add_watch /sys/fs/cgroup/memory/kubepods/burstable/pod926b7ff4-7bff-11e8-945b-52540048533c/6e85761a30707b43ed874e0140f58839618285fc90717153b3cbe7f91629ef5a: no space left on device 系统调用 inotify_add_watch 失败，提示 no space left on device， 这是因为系统上进程 watch 文件目录的总数超出了最大限制，可以修改内核参数调高限制，详细请参考本书 处理实践: inotify watch 耗尽
cgroup 泄露 查看当前 cgroup 数量:</description>
    </item>
    
    <item>
      <title>Node NotReady</title>
      <link>https://k8s.imroc.io/troubleshooting/node/not-ready/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/node/not-ready/</guid>
      <description></description>
    </item>
    
    <item>
      <title>soft lockup (内核软死锁)</title>
      <link>https://k8s.imroc.io/troubleshooting/node/kernel-solft-lockup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://k8s.imroc.io/troubleshooting/node/kernel-solft-lockup/</guid>
      <description>内核报错 Oct 14 15:13:05 VM_1_6_centos kernel: NMI watchdog: BUG: soft lockup - CPU#5 stuck for 22s! [runc:[1:CHILD]:2274] 原因 发生这个报错通常是内核繁忙 (扫描、释放或分配大量对象)，分不出时间片给用户态进程导致的，也伴随着高负载，如果负载降低报错则会消失。
什么情况下会导致内核繁忙  短时间内创建大量进程 (可能是业务需要，也可能是业务bug或用法不正确导致创建大量进程)  参考资料  What are all these &amp;ldquo;Bug: soft lockup&amp;rdquo; messages about : https://www.suse.com/support/kb/doc/?id=7017652  </description>
    </item>
    
  </channel>
</rss>