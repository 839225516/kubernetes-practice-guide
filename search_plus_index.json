{"./":{"url":"./","title":"Introduction","keywords":"","body":"Kubernetes 实践指南 本书包含大量kubernetes实践，涵盖许多问题排查与最佳实践，欢迎大家一起来补充完善。 免责声明 通常文章会先用中文起草并更新，等待其内容较为成熟完善，更新不再频繁的时候才会翻译成英文，点击左上角 \"全球\" 图标切换语言。 LICENSE MIT © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/debug-skill/":{"url":"troubleshooting/debug-skill/","title":"问题定位技巧","keywords":"","body":"问题定位技巧 涵盖许多实用的 Kubernetes 问题定位技巧，帮助你分析解决生产实践中的实际问题，具体内容点击左侧目录树导航栏。 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/debug-skill/analysis-exitcode.html":{"url":"troubleshooting/debug-skill/analysis-exitcode.html","title":"分析 ExitCode 定位 Pod 异常退出原因","keywords":"","body":"分析 ExitCode 定位 Pod 异常退出原因 使用 kubectl describe pod 查看异常 pod 的状态: Containers: kubedns: Container ID: docker://5fb8adf9ee62afc6d3f6f3d9590041818750b392dff015d7091eaaf99cf1c945 Image: ccr.ccs.tencentyun.com/library/kubedns-amd64:1.14.4 Image ID: docker-pullable://ccr.ccs.tencentyun.com/library/kubedns-amd64@sha256:40790881bbe9ef4ae4ff7fe8b892498eecb7fe6dcc22661402f271e03f7de344 Ports: 10053/UDP, 10053/TCP, 10055/TCP Host Ports: 0/UDP, 0/TCP, 0/TCP Args: --domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2 State: Running Started: Tue, 27 Aug 2019 10:58:49 +0800 Last State: Terminated Reason: Error Exit Code: 255 Started: Tue, 27 Aug 2019 10:40:42 +0800 Finished: Tue, 27 Aug 2019 10:58:27 +0800 Ready: True Restart Count: 1 在容器列表里看 Last State 字段，其中 ExitCode 即程序上次退出时的状态码，如果不为 0，表示异常退出，我们可以分析下原因。 退出状态码的区间 必须在 0-255 之间 0 表示正常退出 外界中断将程序退出的时候状态码区间在 129-255，(操作系统给程序发送中断信号，比如 kill -9 是 SIGKILL，ctrl+c 是 SIGINT) 一般程序自身原因导致的异常退出状态区间在 1-128 (这只是一般约定，程序如果一定要用129-255的状态码也是可以的) 假如写代码指定的退出状态码时不在 0-255 之间，例如: exit(-1)，这时会自动做一个转换，最终呈现的状态码还是会在 0-255 之间。我们把状态码记为 code 当指定的退出时状态码为负数，那么转换公式如下: 256 - (|code| % 256) 当指定的退出时状态码为正数，那么转换公式如下: code % 256 常见异常状态码 137 (被 SIGKILL 中断信号杀死) 此状态码一般是因为 pod 中容器内存达到了它的资源限制(resources.limits)，一般是内存溢出(OOM)，CPU达到限制只需要不分时间片给程序就可以。因为限制资源是通过 linux 的 cgroup 实现的，所以 cgroup 会将此容器强制杀掉，类似于 kill -9，此时在 describe pod 中可以看到 Reason 是 OOMKilled 还可能是宿主机本身资源不够用了(OOM)，内核会选取一些进程杀掉来释放内存 不管是 cgroup 限制杀掉进程还是因为节点机器本身资源不够导致进程死掉，都可以从系统日志中找到记录: ubuntu 的系统日志在 /var/log/syslog，centos 的系统日志在 /var/log/messages，都可以用 journalctl -k 来查看系统日志 也可能是 livenessProbe (存活检查) 失败，kubelet 杀死的 pod 还可能是被恶意木马进程杀死 1 和 255 这种可能是一般错误，具体错误原因只能看容器日志，因为很多程序员写异常退出时习惯用 exit(1) 或 exit(-1)，-1 会根据转换规则转成 255 状态码参考 这里罗列了一些状态码的含义：Appendix E. Exit Codes With Special Meanings Linux 标准中断信号 Linux 程序被外界中断时会发送中断信号，程序退出时的状态码就是中断信号值加上 128 得到的，比如 SIGKILL 的中断信号值为 9，那么程序退出状态码就为 9+128=137。以下是标准信号值参考： Signal Value Action Comment ────────────────────────────────────────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating-point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers; see pipe(7) SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 SIGCHLD 20,17,18 Ign Child stopped or terminated SIGCONT 19,18,25 Cont Continue if stopped SIGSTOP 17,19,23 Stop Stop process SIGTSTP 18,20,24 Stop Stop typed at terminal SIGTTIN 21,21,26 Stop Terminal input for background process SIGTTOU 22,22,27 Stop Terminal output for background process C/C++ 退出状态码 /usr/include/sysexits.h 试图将退出状态码标准化(仅限 C/C++): #define EX_OK 0 /* successful termination */ #define EX__BASE 64 /* base value for error messages */ #define EX_USAGE 64 /* command line usage error */ #define EX_DATAERR 65 /* data format error */ #define EX_NOINPUT 66 /* cannot open input */ #define EX_NOUSER 67 /* addressee unknown */ #define EX_NOHOST 68 /* host name unknown */ #define EX_UNAVAILABLE 69 /* service unavailable */ #define EX_SOFTWARE 70 /* internal software error */ #define EX_OSERR 71 /* system error (e.g., can't fork) */ #define EX_OSFILE 72 /* critical OS file missing */ #define EX_CANTCREAT 73 /* can't create (user) output file */ #define EX_IOERR 74 /* input/output error */ #define EX_TEMPFAIL 75 /* temp failure; user is invited to retry */ #define EX_PROTOCOL 76 /* remote error in protocol */ #define EX_NOPERM 77 /* permission denied */ #define EX_CONFIG 78 /* configuration error */ #define EX__MAX 78 /* maximum listed value */ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/debug-skill/capture-packets-in-container.html":{"url":"troubleshooting/debug-skill/capture-packets-in-container.html","title":"容器内抓包定位网络问题","keywords":"","body":"容器内抓包定位网络问题 在使用 kubernetes 跑应用的时候，可能会遇到一些网络问题，比较常见的是服务端无响应(超时)或回包内容不正常，如果没找出各种配置上有问题，这时我们需要确认数据包到底有没有最终被路由到容器里，或者报文到达容器的内容和出容器的内容符不符合预期，通过分析报文可以进一步缩小问题范围。那么如何在容器内抓包呢？本文提供实用的脚本一键进入容器网络命名空间(netns)，使用宿主机上的tcpdump进行抓包。 使用脚本一键进入 pod netns 抓包 发现某个服务不通，最好将其副本数调为1，并找到这个副本 pod 所在节点和 pod 名称 kubectl get pod -o wide 登录 pod 所在节点，将如下脚本粘贴到 shell (注册函数到当前登录的 shell，我们后面用) function e() { set -eu ns=${2-\"default\"} pod=`kubectl -n $ns describe pod $1 | grep -A10 \"^Containers:\" | grep -Eo 'docker://.*$' | head -n 1 | sed 's/docker:\\/\\/\\(.*\\)$/\\1/'` pid=`docker inspect -f {{.State.Pid}} $pod` echo \"entering pod netns for $ns/$1\" cmd=\"nsenter -n --target $pid\" echo $cmd $cmd } 一键进入 pod 所在的 netns，格式：e POD_NAME NAMESPACE，示例： e istio-galley-58c7c7c646-m6568 istio-system e proxy-5546768954-9rxg6 # 省略 NAMESPACE 默认为 default 这时已经进入 pod 的 netns，可以执行宿主机上的 ip a 或 ifconfig 来查看容器的网卡，执行 netstat -tunlp 查看当前容器监听了哪些端口，再通过 tcpdump 抓包： tcpdump -i eth0 -w test.pcap port 80 ctrl-c 停止抓包，再用 scp 或 sz 将抓下来的包下载到本地使用 wireshark 分析，提供一些常用的 wireshark 过滤语法： # 使用 telnet 连上并发送一些测试文本，比如 \"lbtest\"， # 用下面语句可以看发送的测试报文有没有到容器 tcp contains \"lbtest\" # 如果容器提供的是http服务，可以使用 curl 发送一些测试路径的请求， # 通过下面语句过滤 uri 看报文有没有都容器 http.request.uri==\"/mytest\" 脚本原理 我们解释下步骤二中用到的脚本的原理 查看指定 pod 运行的容器 ID kubectl describe pod -n mservice 获得容器进程的 pid docker inspect -f {{.State.Pid}} 进入该容器的 network namespace nsenter -n --target 依赖宿主机的命名：kubectl, docker, nsenter, grep, head, sed © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/debug-skill/use-systemtap-to-locate-problems.html":{"url":"troubleshooting/debug-skill/use-systemtap-to-locate-problems.html","title":"使用 systemtap 定位疑难杂症","keywords":"","body":"使用 systemtap 定位疑难杂症 安装 Ubuntu 安装 systemtap: apt install -y systemtap 运行 stap-prep 检查还有什么需要安装: $ stap-prep Please install linux-headers-4.4.0-104-generic You need package linux-image-4.4.0-104-generic-dbgsym but it does not seem to be available Ubuntu -dbgsym packages are typically in a separate repository Follow https://wiki.ubuntu.com/DebuggingProgramCrash to add this repository apt install -y linux-headers-4.4.0-104-generic 提示需要 dbgsym 包但当前已有软件源中并不包含，需要使用第三方软件源安装，下面是 dbgsym 安装方法(参考官方wiki: https://wiki.ubuntu.com/Kernel/Systemtap): sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys C8CAB6595FDFF622 codename=$(lsb_release -c | awk '{print $2}') sudo tee /etc/apt/sources.list.d/ddebs.list 配置好源后再运行下 stap-prep: $ stap-prep Please install linux-headers-4.4.0-104-generic Please install linux-image-4.4.0-104-generic-dbgsym 提示需要装这两个包，我们安装一下: apt install -y linux-image-4.4.0-104-generic-dbgsym apt install -y linux-headers-4.4.0-104-generic CentOS 安装 systemtap: yum install -y systemtap 默认没装 debuginfo，我们需要装一下，添加软件源 /etc/yum.repos.d/CentOS-Debug.repo: [debuginfo] name=CentOS-$releasever - DebugInfo baseurl=http://debuginfo.centos.org/$releasever/$basearch/ gpgcheck=0 enabled=1 protect=1 priority=1 执行 stap-prep (会安装 kernel-debuginfo) 最后检查确保 kernel-debuginfo 和 kernel-devel 均已安装并且版本跟当前内核版本相同，如果有多个版本，就删除跟当前内核版本不同的包(通过uname -r查看当前内核版本)。 重点检查是否有多个版本的 kernel-devel: $ rpm -qa | grep kernel-devel kernel-devel-3.10.0-327.el7.x86_64 kernel-devel-3.10.0-514.26.2.el7.x86_64 kernel-devel-3.10.0-862.9.1.el7.x86_64 如果存在多个，保证只留跟当前内核版本相同的那个，假设当前内核版本是 3.10.0-862.9.1.el7.x86_64，那么使用 rpm 删除多余的版本: rpm -e kernel-devel-3.10.0-327.el7.x86_64 kernel-devel-3.10.0-514.26.2.el7.x86_64 使用 systemtap 揪出杀死容器的真凶 Pod 莫名其妙被杀死? 可以使用 systemtap 来监视进程的信号发送，原理是 systemtap 将脚本翻译成 C 代码然后调用 gcc 编译成 linux 内核模块，再通过 modprobe 加载到内核，根据脚本内容在内核做各种 hook，在这里我们就 hook 一下信号的发送，找出是谁 kill 掉了容器进程。 首先，找到被杀死的 pod 又自动重启的容器的当前 pid，describe 一下 pod: ...... Container ID: docker://5fb8adf9ee62afc6d3f6f3d9590041818750b392dff015d7091eaaf99cf1c945 ...... Last State: Terminated Reason: Error Exit Code: 137 Started: Thu, 05 Sep 2019 19:22:30 +0800 Finished: Thu, 05 Sep 2019 19:33:44 +0800 拿到容器 id 反查容器的主进程 pid: $ docker inspect -f \"{{.State.Pid}}\" 5fb8adf9ee62afc6d3f6f3d9590041818750b392dff015d7091eaaf99cf1c945 7942 通过 Exit Code 可以看出容器上次退出的状态码，如果进程是被外界中断信号杀死的，退出状态码将在 129-255 之间，137 表示进程是被 SIGKILL 信号杀死的，但我们从这里并不能看出是被谁杀死的。 如果问题可以复现，我们可以使用下面的 systemtap 脚本来监视容器是被谁杀死的(保存为sg.stp): global target_pid = 7942 probe signal.send{ if (sig_pid == target_pid) { printf(\"%s(%d) send %s to %s(%d)\\n\", execname(), pid(), sig_name, pid_name, sig_pid); printf(\"parent of sender: %s(%d)\\n\", pexecname(), ppid()) printf(\"task_ancestry:%s\\n\", task_ancestry(pid2task(pid()), 1)); } } 变量 pid 的值替换为查到的容器主进程 pid 运行脚本: stap sg.stp 当容器进程被杀死时，脚本捕捉到事件，执行输出: pkill(23549) send SIGKILL to server(7942) parent of sender: bash(23495) task_ancestry:swapper/0(0m0.000000000s)=>systemd(0m0.080000000s)=>vGhyM0(19491m2.579563677s)=>sh(33473m38.074571885s)=>bash(33473m38.077072025s)=>bash(33473m38.081028267s)=>bash(33475m4.817798337s)=>pkill(33475m5.202486630s) 通过观察 task_ancestry 可以看到杀死进程的所有父进程，在这里可以看到有个叫 vGhyM0 的奇怪进程名，通常是中了木马，需要安全专家介入继续排查。 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/healthcheck-failed.html":{"url":"troubleshooting/pod/healthcheck-failed.html","title":"健康检查失败","keywords":"","body":"健康检查失败 Kubernetes 健康检查包含就绪检查(readinessProbe)和存活检查(livenessProbe) pod 如果就绪检查失败会将此 pod ip 从 service 中摘除，通过 service 访问，流量将不会被转发给就绪检查失败的 pod pod 如果存活检查失败，kubelet 将会杀死容器并尝试重启 健康检查失败的可能原因有多种，下面我们来逐个排查。 健康检查配置不合理 initialDelaySeconds 太短，容器启动慢，导致容器还没完全启动就开始探测，如果 successThreshold 是默认值 1，检查失败一次就会被 kill，然后 pod 一直这样被 kill 重启。 TODO 节点负载过高 cpu 占用高（比如跑满）会导致进程无法正常发包收包，通常会 timeout，导致 kubelet 认为 pod 不健康。参考本书 节点高负载 一节。 TODO 容器进程被木马进程杀死 TODO 容器内进程端口监听挂掉 使用 netstat -tunlp 检查端口监听是否还在，如果不在了会直接 reset 掉健康检查探测的连接: 20:15:17.890996 IP 172.16.2.1.38074 > 172.16.2.23.8888: Flags [S], seq 96880261, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0 20:15:17.891021 IP 172.16.2.23.8888 > 172.16.2.1.38074: Flags [R.], seq 0, ack 96880262, win 0, length 0 20:15:17.906744 IP 10.0.0.16.54132 > 172.16.2.23.8888: Flags [S], seq 1207014342, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0 20:15:17.906766 IP 172.16.2.23.8888 > 10.0.0.16.54132: Flags [R.], seq 0, ack 1207014343, win 0, length 0 连接异常，从而健康检查失败 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/pod-restart.html":{"url":"troubleshooting/pod/pod-restart.html","title":"Pod 异常重启","keywords":"","body":"Pod 异常重启 TODO 优化 可能原因: 系统 OOM cgroup OOM 节点高负载 看下 pod 状态: $ kubectl get pod task-process-server-5f5bccc77-vkgr2 NAME READY STATUS RESTARTS AGE task-process-server-5f5bccc77-vkgr2 1/1 Running 128 2d RESTARTS 次数可以看出 pod 被重启的次数，如果看到 pod 状态变 CrashLoopBackOff，然后被自动重新拉起变成 Running，导致这个问题的可能原因有多个，我们来一步步排查。 describe 一下 pod，如果 event 还没被冲刷掉 (k8s默认只保留1小时的 event)，通常可以看到 BackOff 的 event: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning BackOff 15m (x6 over 4h) kubelet, 10.0.8.4 Back-off restarting failed container 再看一下 pod 中容器上次的退出状态: Last State: Terminated Reason: Error Exit Code: 137 Started: Thu, 05 Sep 2019 19:22:30 +0800 Finished: Thu, 05 Sep 2019 19:33:44 +0800 先看下 Reason，如果是 OOMKilled，那说明是由于 OOM 被 kill 的 (通常这种情况 Last State 里也没有 Finished © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/pod-pending-forever.html":{"url":"troubleshooting/pod/pod-pending-forever.html","title":"Pod 一直 Pending","keywords":"","body":"Pod 一直 Pending 资源不够 通过 kubectl describe node 查看 node 资源情况，关注以下信息： Allocatable 表示此节点 k8s 能够申请的资源总和 Allocated resources 表示此节点已分配的资源 前者与后者相减，可得出剩余可申请的资源。如果这个值小于 pod 的 request，就不满足 pod 的资源要求，也就不会调度上去 资源够用，但是未被调度 node 不满足 pod 的 nodeSelector 或 affinity 检查 pod 是否有 nodeSelector 或 affinity（亲和性）的配置，如果有，可能是 node 不满足要求导致无法被调度 旧 pod 无法解挂云盘 可能是 pod 之前在另一个节点，但之前节点或kubelet挂了，现在漂移到新的节点上，但是之前pod挂载了cbs云盘，而由于之前节点或kubelet挂了导致无法对磁盘进行解挂，pod 漂移到新的节点时需要挂载之前的cbs云盘，但由于磁盘未被之前的节点解挂，所以新的节点无法进行挂载导致pod一直pending。 解决方法：在云控制台找到对应的云主机或磁盘，手动对磁盘进行卸载，然后pod自动重启时就可以成功挂载了（也可以delete pod让它立即重新调度） 镜像无法下载 看下 pod 的 event，看下是否是因为网络原因无法下载镜像或者下载私有镜像给的 secret 不对 低版本 kube-scheduler 的 bug 可能是低版本 kube-scheduler 的 bug, 可以升级下调度器版本 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/pod-containercreating-forever.html":{"url":"troubleshooting/pod/pod-containercreating-forever.html","title":"Pod 一直 ContainerCreating","keywords":"","body":"Pod 一直 ContainerCreating 查看 Pod 事件 $ kubectl describe pod/apigateway-6dc48bf8b6-l8xrw -n cn-staging no space left on device ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 2m (x4307 over 16h) kubelet, 10.179.80.31 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod \"apigateway-6dc48bf8b6-l8xrw\": Error response from daemon: mkdir /var/lib/docker/aufs/mnt/1f09d6c1c9f24e8daaea5bf33a4230de7dbc758e3b22785e8ee21e3e3d921214-init: no space left on device node上磁盘满了，无法创建和删除 pod，解决方法参考Kubernetes最佳实践：处理容器数据磁盘被写满 Error syncing pod 可能是节点的内存碎片化严重，导致无法创建pod signal: killed memory limit 单位写错，误将memory的limit单位像request一样设置为小 m，这个单位在memory不适用，应该用Mi或M，会被k8s识别成byte，所以pause容器一起来就会被 cgroup-oom kill 掉，导致pod状态一直处于ContainerCreating controller-manager 异常 查看 master 上 kube-controller-manager 状态，异常的话尝试重启 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/pod-terminating-forever.html":{"url":"troubleshooting/pod/pod-terminating-forever.html","title":"Pod 一直 Terminating","keywords":"","body":"Pod 一直 Terminating 容器数据磁盘被写满 如果 docker 的数据目录所在磁盘被写满，docker 无法正常运行，无法进行删除和创建操作，所以 kubelet 调用 docker 删除容器没反应，看 event 类似这样： Normal Killing 39s (x735 over 15h) kubelet, 10.179.80.31 Killing container with id docker://apigateway:Need to kill Pod 处理建议是参考Kubernetes 最佳实践：处理容器数据磁盘被写满 docker 17 的 bug docker hang 住，没有任何响应，看 event: Warning FailedSync 3m (x408 over 1h) kubelet, 10.179.80.31 error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded 怀疑是17版本dockerd的BUG。可通过 kubectl -n cn-staging delete pod apigateway-6dc48bf8b6-clcwk --force --grace-period=0 强制删除pod，但 docker ps 仍看得到这个容器 处置建议： 升级到docker 18. 该版本使用了新的 containerd，针对很多bug进行了修复。 如果出现terminating状态的话，可以提供让容器专家进行排查，不建议直接强行删除，会可能导致一些业务上问题。 存在 Finalizers k8s 资源的 metadata 里如果存在 finalizers，那么该资源一般是由某程序创建的，并且在其创建的资源的 metadata 里的 finalizers 加了一个它的标识，这意味着这个资源被删除时需要由创建资源的程序来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。 处理建议：kubectl edit 手动编辑资源定义，删掉 finalizers，这时再看下资源，就会发现已经删掉了 k8s 版本过低 之前遇到过使用 v1.8.13 版本的 k8s，kubelet 有时 list-watch 出问题，删除 pod 后 kubelet 没收到事件，导致 kubelet 一直没做删除操作，所以 pod 状态一直是 Terminating © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/pod-cannot-exec-or-logs.html":{"url":"troubleshooting/pod/pod-cannot-exec-or-logs.html","title":"Pod 无法被 exec 和 logs","keywords":"","body":"Pod 无法被 exec 和 logs 通常是 apiserver --> kubelet:10250 之间的网络不通，10250 是 kubelet 提供接口的端口，kubectl exec和kubectl logs 的原理就是 apiserver 调 kubelet，kubelet 再调 dockerd 来实现的，所以要保证 kubelet 10250 端口对 apiserver 放通。 TKE托管集群通常不会出现此情况，master 不受节点安全组限制 如果是TKE独立集群，检查节点安全组是否对master节点放通了 10250 端口，如果没放通会导致 apiserver 无法访问 kubelet 10250 端口，从而导致无法进入容器或查看log(kubectl exec和kubectl logs) 检查防火墙、iptables规则是否对 10250 端口数据包进行了拦截 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/pod/slow-pod-terminating.html":{"url":"troubleshooting/pod/slow-pod-terminating.html","title":"TODO:Pod Terminating 慢","keywords":"","body":"TODO:Pod Terminating 慢 TODO 可能原因: 进程通过 bash -c 启动导致 kill 信号无法透传给业务进程 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/cannot-delete-job.html":{"url":"troubleshooting/cannot-delete-job.html","title":"Job 无法被删除","keywords":"","body":"Job 无法被删除 原因 可能是 k8s 的一个bug: https://github.com/kubernetes/kubernetes/issues/43168 本质上是脏数据问题，Running+Succeed != 期望Completions 数量，低版本 kubectl 不容忍，delete job 的时候打开debug(加-v=8)，会看到kubectl不断在重试，直到达到timeout时间。新版kubectl会容忍这些，删除job时会删除关联的pod 解决方法 升级 kubectl 版本，1.12 以上 低版本 kubectl 删除 job 时带 --cascade=false 参数(如果job关联的pod没删完，加这个参数不会删除关联的pod) kubectl delete job --cascade=false © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/node-notready.html":{"url":"troubleshooting/node/node-notready.html","title":"节点 NotReady","keywords":"","body":"节点 NotReady TODO 优化 查看 Node 事件: kubectl get node -o yaml 看看 ready 状态 提示网络有问题 网络的初始化是在Master中做的，一般都是Master问题 没有找到什么特殊信息 一般需要到节点上看看kubelet或者docker日志 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/no-space-left-on-device.html":{"url":"troubleshooting/node/no-space-left-on-device.html","title":"no space left on device","keywords":"","body":"no space left on device 有时候节点 NotReady， kubelet 日志报 no space left on device 有时候创建 Pod 失败，describe pod 看 event 报 no space left on device 出现这种错误有很多中可能原因，下面我们来根据现象找对应原因。 inotify watch 耗尽 节点 NotReady，kubelet 启动失败，看 kubelet 日志: Jul 18 15:20:58 VM_16_16_centos kubelet[11519]: E0718 15:20:58.280275 11519 raw.go:140] Failed to watch directory \"/sys/fs/cgroup/memory/kubepods\": inotify_add_watch /sys/fs/cgroup/memory/kubepods/burstable/pod926b7ff4-7bff-11e8-945b-52540048533c/6e85761a30707b43ed874e0140f58839618285fc90717153b3cbe7f91629ef5a: no space left on device 系统调用 inotify_add_watch 失败，提示 no space left on device， 这是因为系统上进程 watch 文件目录的总数超出了最大限制，可以修改内核参数调高限制，详细请参考本书内核相关章节的 inotify watch 耗尽 cgroup 泄露 查看当前 cgroup 数量: $ cat /proc/cgroups | column -t #subsys_name hierarchy num_cgroups enabled cpuset 5 29 1 cpu 7 126 1 cpuacct 7 126 1 memory 9 127 1 devices 4 126 1 freezer 2 29 1 net_cls 6 29 1 blkio 10 126 1 perf_event 3 29 1 hugetlb 11 29 1 pids 8 126 1 net_prio 6 29 1 cgroup 子系统目录下面所有每个目录及其子目录都认为是一个独立的 cgroup，所以也可以在文件系统中统计目录数来获取实际 cgroup 数量，通常跟 /proc/cgroups 里面看到的应该一致: $ find -L /sys/fs/cgroup/memory -type d | wc -l 127 当 cgroup 泄露发生时，这里的数量就不是真实的了，低版本内核限制最大 65535 个 cgroup，并且开启 kmem 删除 cgroup 时会泄露，大量创建删除容器后泄露了许多 cgroup，最终总数达到 65535，新建容器创建 cgroup 将会失败，报 no space left on device 详细请参考本书内核相关章节的 cgroup 泄露 磁盘被写满(TODO) pod 启动失败 (状态 CreateContainerError) csi-cephfsplugin-27znb 0/2 CreateContainerError 167 17h Warning Failed 5m1s (x3397 over 17h) kubelet, ip-10-0-151-35.us-west-2.compute.internal (combined from similar events): Error: container create failed: container_linux.go:336: starting container process caused \"process_linux.go:399: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/sys\\\\\\\" to rootfs \\\\\\\"/var/lib/containers/storage/overlay/051e985771cc69f3f699895a1dada9ef6483e912b46a99e004af7bb4852183eb/merged\\\\\\\" at \\\\\\\"/var/lib/containers/storage/overlay/051e985771cc69f3f699895a1dada9ef6483e912b46a99e004af7bb4852183eb/merged/sys\\\\\\\" caused \\\\\\\"no space left on device\\\\\\\"\\\"\" © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/rancher-remove-node-cause-cluster-abnormal.html":{"url":"troubleshooting/node/rancher-remove-node-cause-cluster-abnormal.html","title":"Rancher 清除 Node 导致集群异常","keywords":"","body":"Rancher 清除 Node 导致集群异常 现象 安装了 rancher 的用户，在卸载 rancher 的时候，可能会手动执行 kubectl delete ns local 来删除这个 rancher 创建的 namespace，但直接这样做会导致所有 node 被清除，通过 kubectl get node 获取不到 node。 原因 看了下 rancher 源码，rancher 通过 nodes.management.cattle.io 这个 CRD 存储和管理 node，会给所有 node 创建对应的这个 CRD 资源，metadata 中加入了两个 finalizer，其中 user-node-remove_local 对应的 finalizer 处理逻辑就是删除对应的 k8s node 资源，也就是 delete ns local 时，会尝试删除 nodes.management.cattle.io 这些 CRD 资源，进而触发 rancher 的 finalizer 逻辑去删除对应的 k8s node 资源，从而清空了 node，所以 kubectl get node 就看不到 node 了，集群里的服务就无法被调度。 规避方案 不要在 rancher 组件卸载完之前手动 delete ns local。 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/memory-fragmentation.html":{"url":"troubleshooting/node/memory-fragmentation.html","title":"内存碎片化","keywords":"","body":"内存碎片化 判断是否内存碎片化严重 内存页分配失败，内核日志报类似下面的错： mysqld: page allocation failure. order:4, mode:0x10c0d0 mysqld 是被分配的内存的程序 order 表示需要分配连续页的数量(2^order)，这里 4 表示 2^4=16 个连续的页 mode 是内存分配模式的标识，定义在内核源码文件 include/linux/gfp.h 中，通常是多个标识相与运算的结果，不同版本内核可能不一样，比如在新版内核中 GFP_KERNEL 是 __GFP_RECLAIM | __GFP_IO | __GFP_FS 的运算结果，而 __GFP_RECLAIM 又是 ___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM 的运算结果 当 order 为 0 时，说明系统以及完全没有可用内存了，order 值比较大时，才说明内存碎片化了，无法分配连续的大页内存。 内存碎片化造成的问题 容器启动失败 K8S 会为每个 pod 创建 netns 来隔离 network namespace，内核初始化 netns 时会为其创建 nf_conntrack 表的 cache，需要申请大页内存，如果此时系统内存已经碎片化，无法分配到足够的大页内存内核就会报错(v2.6.33 - v4.6): Unable to to create nf_conn slab cache Pod 状态将会一直在 ContainerCreating，dockerd 启动容器失败，日志报错: Jan 23 14:15:31 dc05 dockerd: time=\"2019-01-23T14:15:31.288446233+08:00\" level=error msg=\"containerd: start container\" error=\"oci runtime error: container_linux.go:247: starting container process caused \\\"process_linux.go:245: running exec setns process for init caused \\\\\\\"exit status 6\\\\\\\"\\\"\\n\" id=5b9be8c5bb121264899fac8d9d36b02150269d41ce96ba6ad36d70b8640cb01c Jan 23 14:15:31 dc05 dockerd: time=\"2019-01-23T14:15:31.317965799+08:00\" level=error msg=\"Create container failed with error: invalid header field value \\\"oci runtime error: container_linux.go:247: starting container process caused \\\\\\\"process_linux.go:245: running exec setns process for init caused \\\\\\\\\\\\\\\"exit status 6\\\\\\\\\\\\\\\"\\\\\\\"\\\\n\\\"\" kubelet 日志报错: Jan 23 14:15:31 dc05 kubelet: E0123 14:15:31.352386 26037 remote_runtime.go:91] RunPodSandbox from runtime service failed: rpc error: code = 2 desc = failed to start sandbox container for pod \"matchdataserver-1255064836-t4b2w\": Error response from daemon: {\"message\":\"invalid header field value \\\"oci runtime error: container_linux.go:247: starting container process caused \\\\\\\"process_linux.go:245: running exec setns process for init caused \\\\\\\\\\\\\\\"exit status 6\\\\\\\\\\\\\\\"\\\\\\\"\\\\n\\\"\"} Jan 23 14:15:31 dc05 kubelet: E0123 14:15:31.352496 26037 kuberuntime_sandbox.go:54] CreatePodSandbox for pod \"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\" failed: rpc error: code = 2 desc = failed to start sandbox container for pod \"matchdataserver-1255064836-t4b2w\": Error response from daemon: {\"message\":\"invalid header field value \\\"oci runtime error: container_linux.go:247: starting container process caused \\\\\\\"process_linux.go:245: running exec setns process for init caused \\\\\\\\\\\\\\\"exit status 6\\\\\\\\\\\\\\\"\\\\\\\"\\\\n\\\"\"} Jan 23 14:15:31 dc05 kubelet: E0123 14:15:31.352518 26037 kuberuntime_manager.go:618] createPodSandbox for pod \"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\" failed: rpc error: code = 2 desc = failed to start sandbox container for pod \"matchdataserver-1255064836-t4b2w\": Error response from daemon: {\"message\":\"invalid header field value \\\"oci runtime error: container_linux.go:247: starting container process caused \\\\\\\"process_linux.go:245: running exec setns process for init caused \\\\\\\\\\\\\\\"exit status 6\\\\\\\\\\\\\\\"\\\\\\\"\\\\n\\\"\"} Jan 23 14:15:31 dc05 kubelet: E0123 14:15:31.352580 26037 pod_workers.go:182] Error syncing pod 485fd485-1ed6-11e9-8661-0a587f8021ea (\"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\"), skipping: failed to \"CreatePodSandbox\" for \"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\" with CreatePodSandboxError: \"CreatePodSandbox for pod \\\"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\\\" failed: rpc error: code = 2 desc = failed to start sandbox container for pod \\\"matchdataserver-1255064836-t4b2w\\\": Error response from daemon: {\\\"message\\\":\\\"invalid header field value \\\\\\\"oci runtime error: container_linux.go:247: starting container process caused \\\\\\\\\\\\\\\"process_linux.go:245: running exec setns process for init caused \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"exit status 6\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\"\\\"}\" Jan 23 14:15:31 dc05 kubelet: I0123 14:15:31.372181 26037 kubelet.go:1916] SyncLoop (PLEG): \"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\", event: &pleg.PodLifecycleEvent{ID:\"485fd485-1ed6-11e9-8661-0a587f8021ea\", Type:\"ContainerDied\", Data:\"5b9be8c5bb121264899fac8d9d36b02150269d41ce96ba6ad36d70b8640cb01c\"} Jan 23 14:15:31 dc05 kubelet: W0123 14:15:31.372225 26037 pod_container_deletor.go:77] Container \"5b9be8c5bb121264899fac8d9d36b02150269d41ce96ba6ad36d70b8640cb01c\" not found in pod's containers Jan 23 14:15:31 dc05 kubelet: I0123 14:15:31.678211 26037 kuberuntime_manager.go:383] No ready sandbox for pod \"matchdataserver-1255064836-t4b2w_basic(485fd485-1ed6-11e9-8661-0a587f8021ea)\" can be found. Need to start a new one 查看slab (后面的0多表示伙伴系统没有大块内存了)： $ cat /proc/buddyinfo Node 0, zone DMA 1 0 1 0 2 1 1 0 1 1 3 Node 0, zone DMA32 2725 624 489 178 0 0 0 0 0 0 0 Node 0, zone Normal 1163 1101 932 222 0 0 0 0 0 0 0 系统 OOM 内存碎片化会导致即使当前系统总内存比较多，但由于无法分配足够的大页内存导致给进程分配内存失败，就认为系统内存不够用，需要杀掉一些进程来释放内存，从而导致系统 OOM 解决方法 周期性地或者在发现大块内存不足时，先进行drop_cache操作: echo 3 > /proc/sys/vm/drop_caches 必要时候进行内存整理，开销会比较大，会造成业务卡住一段时间(慎用): echo 1 > /proc/sys/vm/compact_memory 如何防止内存碎片化 TODO 附录 相关链接： https://huataihuang.gitbooks.io/cloud-atlas/content/os/linux/kernel/memory/drop_caches_and_compact_memory.html © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/high-load-on-node.html":{"url":"troubleshooting/node/high-load-on-node.html","title":"节点高负载","keywords":"","body":"节点高负载 TODO 优化 节点高负载会导致进程无法获得足够的 cpu 时间片来运行，通常表现为网络 timeout，健康检查失败，服务不可用。 过多 IO 等待 有时候即便 cpu ‘us’ (user) 不高但 cpu ‘id’ (idle) 很高的情况节点负载也很高，这是为什么呢？通常是文件 IO 性能达到瓶颈导致 IO WAIT 过多，从而使得节点整体负载升高，影响其它进程的性能。 使用 top 命令看下当前负载： top - 19:42:06 up 23:59, 2 users, load average: 34.64, 35.80, 35.76 Tasks: 679 total, 1 running, 678 sleeping, 0 stopped, 0 zombie Cpu(s): 15.6%us, 1.7%sy, 0.0%ni, 74.7%id, 7.9%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 32865032k total, 30989168k used, 1875864k free, 370748k buffers Swap: 8388604k total, 5440k used, 8383164k free, 7982424k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9783 mysql 20 0 17.3g 16g 8104 S 186.9 52.3 3752:33 mysqld 5700 nginx 20 0 1330m 66m 9496 S 8.9 0.2 0:20.82 php-fpm 6424 nginx 20 0 1330m 65m 8372 S 8.3 0.2 0:04.97 php-fpm 6573 nginx 20 0 1330m 64m 7368 S 8.3 0.2 0:01.49 php-fpm 5927 nginx 20 0 1320m 56m 9272 S 7.6 0.2 0:12.54 php-fpm 5956 nginx 20 0 1330m 65m 8500 S 7.6 0.2 0:12.70 php-fpm 6126 nginx 20 0 1321m 57m 8964 S 7.3 0.2 0:09.72 php-fpm 6127 nginx 20 0 1319m 54m 9520 S 6.6 0.2 0:08.73 php-fpm 6131 nginx 20 0 1320m 56m 9404 S 6.6 0.2 0:09.43 php-fpm 6174 nginx 20 0 1321m 56m 8444 S 6.3 0.2 0:08.92 php-fpm 5790 nginx 20 0 1319m 54m 9468 S 5.6 0.2 0:17.33 php-fpm 6575 nginx 20 0 1320m 55m 8212 S 5.6 0.2 0:02.11 php-fpm 6160 nginx 20 0 1310m 44m 8296 S 4.0 0.1 0:10.05 php-fpm 5597 nginx 20 0 1310m 46m 9556 S 3.6 0.1 0:21.03 php-fpm 5786 nginx 20 0 1310m 45m 8528 S 3.6 0.1 0:15.53 php-fpm 5797 nginx 20 0 1310m 46m 9444 S 3.6 0.1 0:14.02 php-fpm 6158 nginx 20 0 1310m 45m 8324 S 3.6 0.1 0:10.20 php-fpm 5698 nginx 20 0 1310m 46m 9184 S 3.3 0.1 0:20.62 php-fpm 5779 nginx 20 0 1309m 44m 8336 S 3.3 0.1 0:15.34 php-fpm 6540 nginx 20 0 1306m 40m 7884 S 3.3 0.1 0:02.46 php-fpm 5553 nginx 20 0 1300m 36m 9568 S 3.0 0.1 0:21.58 php-fpm 5722 nginx 20 0 1310m 45m 8552 S 3.0 0.1 0:17.25 php-fpm 5920 nginx 20 0 1302m 36m 8208 S 3.0 0.1 0:14.23 php-fpm 6432 nginx 20 0 1310m 45m 8420 S 3.0 0.1 0:05.86 php-fpm 5285 nginx 20 0 1302m 38m 9696 S 2.7 0.1 0:23.41 php-fpm wa (wait) 表示 IO WAIT 的 cpu 占用，默认看到的是所有核的平均值，要看每个核的 wa 值需要按下 \"1\": top - 19:42:08 up 23:59, 2 users, load average: 34.64, 35.80, 35.76 Tasks: 679 total, 1 running, 678 sleeping, 0 stopped, 0 zombie Cpu0 : 29.5%us, 3.7%sy, 0.0%ni, 48.7%id, 17.9%wa, 0.0%hi, 0.1%si, 0.0%st Cpu1 : 29.3%us, 3.7%sy, 0.0%ni, 48.9%id, 17.9%wa, 0.0%hi, 0.1%si, 0.0%st Cpu2 : 26.1%us, 3.1%sy, 0.0%ni, 64.4%id, 6.0%wa, 0.0%hi, 0.3%si, 0.0%st Cpu3 : 25.9%us, 3.1%sy, 0.0%ni, 65.5%id, 5.4%wa, 0.0%hi, 0.1%si, 0.0%st Cpu4 : 24.9%us, 3.0%sy, 0.0%ni, 66.8%id, 5.0%wa, 0.0%hi, 0.3%si, 0.0%st Cpu5 : 24.9%us, 2.9%sy, 0.0%ni, 67.0%id, 4.8%wa, 0.0%hi, 0.3%si, 0.0%st Cpu6 : 24.2%us, 2.7%sy, 0.0%ni, 68.3%id, 4.5%wa, 0.0%hi, 0.3%si, 0.0%st Cpu7 : 24.3%us, 2.6%sy, 0.0%ni, 68.5%id, 4.2%wa, 0.0%hi, 0.3%si, 0.0%st Cpu8 : 23.8%us, 2.6%sy, 0.0%ni, 69.2%id, 4.1%wa, 0.0%hi, 0.3%si, 0.0%st Cpu9 : 23.9%us, 2.5%sy, 0.0%ni, 69.3%id, 4.0%wa, 0.0%hi, 0.3%si, 0.0%st Cpu10 : 23.3%us, 2.4%sy, 0.0%ni, 68.7%id, 5.6%wa, 0.0%hi, 0.0%si, 0.0%st Cpu11 : 23.3%us, 2.4%sy, 0.0%ni, 69.2%id, 5.1%wa, 0.0%hi, 0.0%si, 0.0%st Cpu12 : 21.8%us, 2.4%sy, 0.0%ni, 60.2%id, 15.5%wa, 0.0%hi, 0.0%si, 0.0%st Cpu13 : 21.9%us, 2.4%sy, 0.0%ni, 60.6%id, 15.2%wa, 0.0%hi, 0.0%si, 0.0%st Cpu14 : 21.4%us, 2.3%sy, 0.0%ni, 72.6%id, 3.7%wa, 0.0%hi, 0.0%si, 0.0%st Cpu15 : 21.5%us, 2.2%sy, 0.0%ni, 73.2%id, 3.1%wa, 0.0%hi, 0.0%si, 0.0%st Cpu16 : 21.2%us, 2.2%sy, 0.0%ni, 73.6%id, 3.0%wa, 0.0%hi, 0.0%si, 0.0%st Cpu17 : 21.2%us, 2.1%sy, 0.0%ni, 73.8%id, 2.8%wa, 0.0%hi, 0.0%si, 0.0%st Cpu18 : 20.9%us, 2.1%sy, 0.0%ni, 74.1%id, 2.9%wa, 0.0%hi, 0.0%si, 0.0%st Cpu19 : 21.0%us, 2.1%sy, 0.0%ni, 74.4%id, 2.5%wa, 0.0%hi, 0.0%si, 0.0%st Cpu20 : 20.7%us, 2.0%sy, 0.0%ni, 73.8%id, 3.4%wa, 0.0%hi, 0.0%si, 0.0%st Cpu21 : 20.8%us, 2.0%sy, 0.0%ni, 73.9%id, 3.2%wa, 0.0%hi, 0.0%si, 0.0%st Cpu22 : 20.8%us, 2.0%sy, 0.0%ni, 74.4%id, 2.8%wa, 0.0%hi, 0.0%si, 0.0%st Cpu23 : 20.8%us, 1.9%sy, 0.0%ni, 74.4%id, 2.8%wa, 0.0%hi, 0.0%si, 0.0%st Mem: 32865032k total, 30209248k used, 2655784k free, 370748k buffers Swap: 8388604k total, 5440k used, 8383164k free, 7986552k cached wa 通常是 0%，如果经常在 1 之上，说明存储设备的速度已经太慢，无法跟上 cpu 的处理速度。 使用 atop 看下当前磁盘 IO 状态: ATOP - lemp 2017/01/23 19:42:32 --------- 10s elapsed PRC | sys 3.18s | user 33.24s | #proc 679 | #tslpu 28 | #zombie 0 | #exit 0 | CPU | sys 29% | user 330% | irq 1% | idle 1857% | wait 182% | curscal 69% | CPL | avg1 33.00 | avg5 35.29 | avg15 35.59 | csw 62610 | intr 76926 | numcpu 24 | MEM | tot 31.3G | free 2.1G | cache 7.6G | dirty 41.0M | buff 362.1M | slab 1.2G | SWP | tot 8.0G | free 8.0G | | | vmcom 23.9G | vmlim 23.7G | DSK | sda | busy 100% | read 4 | write 1789 | MBw/s 2.84 | avio 5.58 ms | NET | transport | tcpi 10357 | tcpo 9065 | udpi 0 | udpo 0 | tcpao 174 | NET | network | ipi 10360 | ipo 9065 | ipfrw 0 | deliv 10359 | icmpo 0 | NET | eth0 4% | pcki 6649 | pcko 6136 | si 1478 Kbps | so 4115 Kbps | erro 0 | NET | lo ---- | pcki 4082 | pcko 4082 | si 8967 Kbps | so 8967 Kbps | erro 0 | PID TID THR SYSCPU USRCPU VGROW RGROW RDDSK WRDSK ST EXC S CPUNR CPU CMD 1/12 9783 - 156 0.21s 19.44s 0K -788K 4K 1344K -- - S 4 197% mysqld 5596 - 1 0.10s 0.62s 47204K 47004K 0K 220K -- - S 18 7% php-fpm 6429 - 1 0.06s 0.34s 19840K 19968K 0K 0K -- - S 21 4% php-fpm 6210 - 1 0.03s 0.30s -5216K -5204K 0K 0K -- - S 19 3% php-fpm 5757 - 1 0.05s 0.27s 26072K 26012K 0K 4K -- - S 13 3% php-fpm 6433 - 1 0.04s 0.28s -2816K -2816K 0K 0K -- - S 11 3% php-fpm 5846 - 1 0.06s 0.22s -2560K -2660K 0K 0K -- - S 7 3% php-fpm 5791 - 1 0.05s 0.21s 5764K 5692K 0K 0K -- - S 22 3% php-fpm 5860 - 1 0.04s 0.21s 48088K 47724K 0K 0K -- - S 1 3% php-fpm 6231 - 1 0.04s 0.20s -256K -4K 0K 0K -- - S 1 2% php-fpm 6154 - 1 0.03s 0.21s -3004K -3184K 0K 0K -- - S 21 2% php-fpm 6573 - 1 0.04s 0.20s -512K -168K 0K 0K -- - S 4 2% php-fpm 6435 - 1 0.04s 0.19s -3216K -2980K 0K 0K -- - S 15 2% php-fpm 5954 - 1 0.03s 0.20s 0K 164K 0K 4K -- - S 0 2% php-fpm 6133 - 1 0.03s 0.19s 41056K 40432K 0K 0K -- - S 18 2% php-fpm 6132 - 1 0.02s 0.20s 37836K 37440K 0K 0K -- - S 11 2% php-fpm 6242 - 1 0.03s 0.19s -12.2M -12.3M 0K 4K -- - S 12 2% php-fpm 6285 - 1 0.02s 0.19s 39516K 39420K 0K 0K -- - S 3 2% php-fpm 6455 - 1 0.05s 0.16s 29008K 28560K 0K 0K -- - S 14 2% php-fpm 在本例中磁盘 sda 已经 100% busy，已经严重达到性能瓶颈。按 'd' 看下是哪些进程在使用磁盘IO: ATOP - lemp 2017/01/23 19:42:46 --------- 2s elapsed PRC | sys 0.24s | user 1.99s | #proc 679 | #tslpu 54 | #zombie 0 | #exit 0 | CPU | sys 11% | user 101% | irq 1% | idle 2089% | wait 208% | curscal 63% | CPL | avg1 38.49 | avg5 36.48 | avg15 35.98 | csw 4654 | intr 6876 | numcpu 24 | MEM | tot 31.3G | free 2.2G | cache 7.6G | dirty 48.7M | buff 362.1M | slab 1.2G | SWP | tot 8.0G | free 8.0G | | | vmcom 23.9G | vmlim 23.7G | DSK | sda | busy 100% | read 2 | write 362 | MBw/s 2.28 | avio 5.49 ms | NET | transport | tcpi 1031 | tcpo 968 | udpi 0 | udpo 0 | tcpao 45 | NET | network | ipi 1031 | ipo 968 | ipfrw 0 | deliv 1031 | icmpo 0 | NET | eth0 1% | pcki 558 | pcko 508 | si 762 Kbps | so 1077 Kbps | erro 0 | NET | lo ---- | pcki 406 | pcko 406 | si 2273 Kbps | so 2273 Kbps | erro 0 | PID TID RDDSK WRDSK WCANCL DSK CMD 1/5 9783 - 0K 468K 16K 40% mysqld 1930 - 0K 212K 0K 18% flush-8:0 5896 - 0K 152K 0K 13% nginx 880 - 0K 148K 0K 13% jbd2/sda5-8 5909 - 0K 60K 0K 5% nginx 5906 - 0K 36K 0K 3% nginx 5907 - 16K 8K 0K 2% nginx 5903 - 20K 0K 0K 2% nginx 5901 - 0K 12K 0K 1% nginx 5908 - 0K 8K 0K 1% nginx 5894 - 0K 8K 0K 1% nginx 5911 - 0K 8K 0K 1% nginx 5900 - 0K 4K 4K 0% nginx 5551 - 0K 4K 0K 0% php-fpm 5913 - 0K 4K 0K 0% nginx 5895 - 0K 4K 0K 0% nginx 6133 - 0K 0K 0K 0% php-fpm 5780 - 0K 0K 0K 0% php-fpm 6675 - 0K 0K 0K 0% atop 也可以使用 iotop -oPa 查看哪些进程占用磁盘 IO: Total DISK READ: 15.02 K/s | Total DISK WRITE: 3.82 M/s PID PRIO USER DISK READ DISK WRITE SWAPIN IO> COMMAND 1930 be/4 root 0.00 B 1956.00 K 0.00 % 83.34 % [flush-8:0] 5914 be/4 nginx 0.00 B 0.00 B 0.00 % 36.56 % nginx: cache manager process 880 be/3 root 0.00 B 21.27 M 0.00 % 35.03 % [jbd2/sda5-8] 5913 be/2 nginx 36.00 K 1000.00 K 0.00 % 8.94 % nginx: worker process 5910 be/2 nginx 0.00 B 1048.00 K 0.00 % 8.43 % nginx: worker process 5896 be/2 nginx 56.00 K 452.00 K 0.00 % 6.91 % nginx: worker process 5909 be/2 nginx 20.00 K 1144.00 K 0.00 % 6.24 % nginx: worker process 5890 be/2 nginx 48.00 K 692.00 K 0.00 % 6.07 % nginx: worker process 5892 be/2 nginx 84.00 K 736.00 K 0.00 % 5.71 % nginx: worker process 5901 be/2 nginx 20.00 K 504.00 K 0.00 % 5.46 % nginx: worker process 5899 be/2 nginx 0.00 B 596.00 K 0.00 % 5.14 % nginx: worker process 5897 be/2 nginx 28.00 K 1388.00 K 0.00 % 4.90 % nginx: worker process 5908 be/2 nginx 48.00 K 700.00 K 0.00 % 4.43 % nginx: worker process 5905 be/2 nginx 32.00 K 1140.00 K 0.00 % 4.36 % nginx: worker process 5900 be/2 nginx 0.00 B 1208.00 K 0.00 % 4.31 % nginx: worker process 5904 be/2 nginx 36.00 K 1244.00 K 0.00 % 2.80 % nginx: worker process 5895 be/2 nginx 16.00 K 780.00 K 0.00 % 2.50 % nginx: worker process 5907 be/2 nginx 0.00 B 1548.00 K 0.00 % 2.43 % nginx: worker process 5903 be/2 nginx 36.00 K 1032.00 K 0.00 % 2.34 % nginx: worker process 6130 be/4 nginx 0.00 B 72.00 K 0.00 % 2.18 % php-fpm: pool www 5906 be/2 nginx 12.00 K 844.00 K 0.00 % 2.10 % nginx: worker process 5889 be/2 nginx 40.00 K 1164.00 K 0.00 % 2.00 % nginx: worker process 5894 be/2 nginx 44.00 K 760.00 K 0.00 % 1.61 % nginx: worker process 5902 be/2 nginx 52.00 K 992.00 K 0.00 % 1.55 % nginx: worker process 5893 be/2 nginx 64.00 K 972.00 K 0.00 % 1.22 % nginx: worker process 5814 be/4 nginx 36.00 K 44.00 K 0.00 % 1.06 % php-fpm: pool www 6159 be/4 nginx 4.00 K 4.00 K 0.00 % 1.00 % php-fpm: pool www 5693 be/4 nginx 0.00 B 4.00 K 0.00 % 0.86 % php-fpm: pool www 5912 be/2 nginx 68.00 K 300.00 K 0.00 % 0.72 % nginx: worker process 5911 be/2 nginx 20.00 K 788.00 K 0.00 % 0.72 % nginx: worker process 通过 man iotop 可以看下这几个参数的含义： -o, --only Only show processes or threads actually doing I/O, instead of showing all processes or threads. This can be dynamically toggled by pressing o. -P, --processes Only show processes. Normally iotop shows all threads. -a, --accumulated Show accumulated I/O instead of bandwidth. In this mode, iotop shows the amount of I/O processes have done since iotop started. 节点上部署了其它非 K8S 管理的服务 TODO 优化 比如在节点上装了数据库，但不被 K8S 所管理，这是用法不正确，不建议在 K8S 节点上部署其它进程。 参考资料 Linux server performance: Is disk I/O slowing your application: https://haydenjames.io/linux-server-performance-disk-io-slowing-application/ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/eviction-leads-to-service-disruption.html":{"url":"troubleshooting/node/eviction-leads-to-service-disruption.html","title":"驱逐导致服务中断","keywords":"","body":"驱逐导致服务中断 TODO 优化 案例 TKE 一客户的某个节点有问题，无法挂载nfs，通过新加节点，驱逐故障节点的 pod 来规避，但导致了业务 10min 服务不可用，排查发现其它节点 pod 很多集体出现了重启，主要是连不上 kube-dns 无法解析 service，业务调用不成功，从而对外表现为服务不可用。 为什么会中断？驱逐的原理是先封锁节点，然后将旧的 node 上的 pod 删除，replicaset 控制器检测到 pod 减少，会重新创建一个 pod，调度到新的 node上，这个过程是先删除，再创建，并非是滚动更新，因此更新过程中，如果一个deployment的所有 pod 都在被驱逐的节点上，则可能导致该服务不可用。 那为什么会影响其它 pod？分析kubelet日志，kube-dns 有两个副本，都在这个被驱逐的节点上，所以驱逐的时候 kube-dns 不通，影响了其它 pod 解析 service，导致服务集体不可用。 那为什么会中断这么久？通常在新的节点应该很会快才是，通过进一步分析新节点的 kubelet 日志，发现 kube-dns 从拉镜像到容器启动之间花了很长时间，检查节点上的镜像发现有很多大镜像(1~2GB)，猜测是拉取镜像有并发限制，kube-dns 的镜像虽小，但在排队等大镜像下载完，检查 kubelet 启动参数，确实有 --registry-burst 这个参数控制镜像下载并发数限制。但最终发现其实应该是 --serialize-image-pulls 这个参数导致的，kubelet 启动参数没有指定该参数，而该参数默认值为 true，即默认串行下载镜像，不并发下载，所以导致镜像下载排队，是的 kube-dns 延迟了很长时间才启动。 解决方案 避免服务单点故障，多副本，并加反亲和性 设置 preStop hook 与 readinessProbe，更新路由规则 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/cgroup-leaking.html":{"url":"troubleshooting/node/cgroup-leaking.html","title":"cgroup 泄露","keywords":"","body":"cgroup 泄露 内核 Bug memcg 是 Linux 内核中用于管理 cgroup 内存的模块，整个生命周期应该是跟随 cgroup 的，但是在低版本内核中(已知3.10)，一旦给某个 memory cgroup 开启 kmem accounting 中的 memory.kmem.limit_in_bytes 就可能会导致不能彻底删除 memcg 和对应的 cssid，也就是说应用即使已经删除了 cgroup (/sys/fs/cgroup/memory 下对应的 cgroup 目录已经删除), 但在内核中没有释放 cssid，导致内核认为的 cgroup 的数量实际数量不一致，我们也无法得知内核认为的 cgroup 数量是多少。 关于 cgroup kernel memory，在 kernel.org 中有如下描述： 2.7 Kernel Memory Extension (CONFIG_MEMCG_KMEM) ----------------------------------------------- With the Kernel memory extension, the Memory Controller is able to limit the amount of kernel memory used by the system. Kernel memory is fundamentally different than user memory, since it can't be swapped out, which makes it possible to DoS the system by consuming too much of this precious resource. Kernel memory accounting is enabled for all memory cgroups by default. But it can be disabled system-wide by passing cgroup.memory=nokmem to the kernel at boot time. In this case, kernel memory will not be accounted at all. Kernel memory limits are not imposed for the root cgroup. Usage for the root cgroup may or may not be accounted. The memory used is accumulated into memory.kmem.usage_in_bytes, or in a separate counter when it makes sense. (currently only for tcp). The main \"kmem\" counter is fed into the main counter, so kmem charges will also be visible from the user counter. Currently no soft limit is implemented for kernel memory. It is future work to trigger slab reclaim when those limits are reached. 这是一个 cgroup memory 的扩展，用于限制对 kernel memory 的使用，但该特性在老于 4.0 版本中是个实验特性，存在泄露问题，在 4.x 较低的版本也还有泄露问题，应该是造成泄露的代码路径没有完全修复，推荐 4.3 以上的内核。 造成容器创建失败 这个问题可能会导致创建容器失败，因为创建容器为其需要创建 cgroup 来做隔离，而低版本内核有个限制：允许创建的 cgroup 最大数量写死为 65535 (点我跳转到 commit)，如果节点上经常创建和销毁大量容器导致创建很多 cgroup，删除容器但没有彻底删除 cgroup 造成泄露(真实数量我们无法得知)，到达 65535 后再创建容器就会报创建 cgroup 失败并报错 no space left on device，使用 kubernetes 最直观的感受就是 pod 创建之后无法启动成功。 pod 启动失败，报 event 示例: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 15m default-scheduler Successfully assigned jenkins/jenkins-7845b9b665-nrvks to 10.10.252.4 Warning FailedCreatePodContainer 25s (x70 over 15m) kubelet, 10.10.252.4 unable to ensure pod container exists: failed to create container for [kubepods besteffort podc6eeec88-8664-11e9-9524-5254007057ba] : mkdir /sys/fs/cgroup/memory/kubepods/besteffort/podc6eeec88-8664-11e9-9524-5254007057ba: no space left on device dockerd 日志报错示例: Dec 24 11:54:31 VM_16_11_centos dockerd[11419]: time=\"2018-12-24T11:54:31.195900301+08:00\" level=error msg=\"Handler for POST /v1.31/containers/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b/start returned error: OCI runtime create failed: container_linux.go:348: starting container process caused \\\"process_linux.go:279: applying cgroup configuration for process caused \\\\\\\"mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod79fe803c-072f-11e9-90ca-525400090c71/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b: no space left on device\\\\\\\"\\\": unknown\" kubelet 日志报错示例: Sep 09 18:09:09 VM-0-39-ubuntu kubelet[18902]: I0909 18:09:09.449722 18902 remote_runtime.go:92] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"osp-xxx-com-ljqm19-54bf7678b8-bvz9s\": Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:258: applying cgroup configuration for process caused \\\"mkdir /sys/fs/cgroup/memory/kubepods/burstable/podf1bd9e87-1ef2-11e8-afd3-fa163ecf2dce/8710c146b3c8b52f5da62e222273703b1e3d54a6a6270a0ea7ce1b194f1b5053: no space left on device\\\"\" 新版的内核限制为 2^31 (可以看成几乎不限制，点我跳转到代码): cgroup_idr_alloc() 传入 end 为 0 到 idr_alloc()， 再传给 idr_alloc_u32(), end 的值最终被三元运算符 end>0 ? end-1 : INT_MAX 转成了 INT_MAX 常量，即 2^31。所以如果新版内核有泄露问题会更难定位，表现形式会是内存消耗严重，幸运的是新版内核已经修复，推荐 4.3 以上。 规避方案 如果你用的低版本内核(比如 CentOS 7 v3.10 的内核)并且不方便升级内核，可以通过不开启 kmem accounting 来实现规避，但会比较麻烦。 kubelet 和 runc 都会给 memory cgroup 开启 kmem accounting，所以要规避这个问题，就要保证kubelet 和 runc 都别开启 kmem accounting，下面分别进行说明: runc runc 在合并 这个PR (2017-02-27) 之后创建的容器都默认开启了 kmem accounting，后来社区也注意到这个问题，并做了比较灵活的修复， PR 1921 给 runc 增加了 \"nokmem\" 编译选项，缺省的 release 版本没有使用这个选项， 自己使用 nokmem 选项编译 runc 的方法: cd $GO_PATH/src/github.com/opencontainers/runc/ make BUILDTAGS=\"seccomp nokmem\" docker-ce v18.09.1 之后的 runc 默认关闭了 kmem accounting，所以也可以直接升级 docker 到这个版本之后。 kubelet 如果是 1.14 版本及其以上，可以在编译的时候通过 build tag 来关闭 kmem accounting: KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=\"-tags=nokmem\" 如果是低版本需要修改代码重新编译。kubelet 在创建 pod 对应的 cgroup 目录时，也会调用 libcontianer 中的代码对 cgroup 做设置，在 pkg/kubelet/cm/cgroup_manager_linux.go 的 Create 方法中，会调用 Manager.Apply 方法，最终调用 vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go 中的 MemoryGroup.Apply 方法，开启 kmem accounting。这里也需要进行处理，可以将这部分代码注释掉然后重新编译 kubelet。 参考资料 一行 kubernetes 1.9 代码引发的血案（与 CentOS 7.x 内核兼容性问题）: http://dockone.io/article/4797 Cgroup泄漏--潜藏在你的集群中: https://tencentcloudcontainerteam.github.io/2018/12/29/cgroup-leaking/ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/runnig-out-of-inotify-watches.html":{"url":"troubleshooting/node/runnig-out-of-inotify-watches.html","title":"inotify watch 耗尽","keywords":"","body":"inotify watch 耗尽 每个 linux 进程可以持有多个 fd，每个 inotify 类型的 fd 可以 watch 多个目录，每个用户下所有进程 inotify 类型的 fd 可以 watch 的总目录数有个最大限制，这个限制可以通过内核参数配置: fs.inotify.max_user_watches 查看最大 inotify watch 数: $ cat /proc/sys/fs/inotify/max_user_watches 8192 使用下面的脚本查看当前有 inotify watch 类型 fd 的进程以及每个 fd watch 的目录数量，降序输出，带总数统计: #!/usr/bin/env bash # # Copyright 2019 (c) roc # # This script shows processes holding the inotify fd, alone with HOW MANY directories each inotify fd watches(0 will be ignored). total=0 result=\"EXE PID FD-INFO INOTIFY-WATCHES\\n\" while read pid fd; do \\ exe=\"$(readlink -f /proc/$pid/exe || echo n/a)\"; \\ fdinfo=\"/proc/$pid/fdinfo/$fd\" ; \\ count=\"$(grep -c inotify \"$fdinfo\" || true)\"; \\ if [ $((count)) != 0 ]; then total=$((total+count)); \\ result+=\"$exe $pid $fdinfo $count\\n\"; \\ fi done 示例输出: total 7882 inotify watches EXE PID FD-INFO INOTIFY-WATCHES /usr/local/qcloud/YunJing/YDEyes/YDService 25813 /proc/25813/fdinfo/8 7077 /usr/bin/kubelet 1173 /proc/1173/fdinfo/22 665 /usr/bin/ruby2.3 13381 /proc/13381/fdinfo/14 54 /usr/lib/policykit-1/polkitd 1458 /proc/1458/fdinfo/9 14 /lib/systemd/systemd-udevd 450 /proc/450/fdinfo/9 13 /usr/sbin/nscd 7935 /proc/7935/fdinfo/3 6 /usr/bin/kubelet 1173 /proc/1173/fdinfo/28 5 /lib/systemd/systemd 1 /proc/1/fdinfo/17 4 /lib/systemd/systemd 1 /proc/1/fdinfo/18 4 /lib/systemd/systemd 1 /proc/1/fdinfo/26 4 /lib/systemd/systemd 1 /proc/1/fdinfo/28 4 /usr/lib/policykit-1/polkitd 1458 /proc/1458/fdinfo/8 4 /usr/local/bin/sidecar-injector 4751 /proc/4751/fdinfo/3 3 /usr/lib/accountsservice/accounts-daemon 1178 /proc/1178/fdinfo/7 2 /usr/local/bin/galley 8228 /proc/8228/fdinfo/10 2 /usr/local/bin/galley 8228 /proc/8228/fdinfo/9 2 /lib/systemd/systemd 1 /proc/1/fdinfo/11 1 /sbin/agetty 1437 /proc/1437/fdinfo/4 1 /sbin/agetty 1440 /proc/1440/fdinfo/4 1 /usr/bin/kubelet 1173 /proc/1173/fdinfo/10 1 /usr/local/bin/envoy 4859 /proc/4859/fdinfo/5 1 /usr/local/bin/envoy 5427 /proc/5427/fdinfo/5 1 /usr/local/bin/envoy 6058 /proc/6058/fdinfo/3 1 /usr/local/bin/envoy 6893 /proc/6893/fdinfo/3 1 /usr/local/bin/envoy 6950 /proc/6950/fdinfo/3 1 /usr/local/bin/galley 8228 /proc/8228/fdinfo/3 1 /usr/local/bin/pilot-agent 3819 /proc/3819/fdinfo/5 1 /usr/local/bin/pilot-agent 4244 /proc/4244/fdinfo/5 1 /usr/local/bin/pilot-agent 5901 /proc/5901/fdinfo/3 1 /usr/local/bin/pilot-agent 6789 /proc/6789/fdinfo/3 1 /usr/local/bin/pilot-agent 6808 /proc/6808/fdinfo/3 1 /usr/local/bin/pilot-discovery 6231 /proc/6231/fdinfo/3 1 /usr/local/bin/sidecar-injector 4751 /proc/4751/fdinfo/5 1 /usr/sbin/acpid 1166 /proc/1166/fdinfo/6 1 /usr/sbin/dnsmasq 7572 /proc/7572/fdinfo/8 1 如果看到总 watch 数比较大，接近最大限制，可以修改内核参数调高下这个限制。 临时调整: sudo sysctl fs.inotify.max_user_watches=524288 永久生效: echo \"fs.inotify.max_user_watches=524288\" >> /etc/sysctl.conf && sysctl -p 打开 inotify_add_watch 跟踪，进一步 debug inotify watch 耗尽的原因: echo 1 >> /sys/kernel/debug/tracing/events/syscalls/sys_exit_inotify_add_watch/enable © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/lost-packets-in-nat-environment-once-enable-tcp_tw_recycle.html":{"url":"troubleshooting/node/lost-packets-in-nat-environment-once-enable-tcp_tw_recycle.html","title":"tcp_tw_recycle 导致在 NAT 环境会丢包","keywords":"","body":"tcp_tw_recycle 导致在 NAT 环境会丢包 tcp_tw_recycle 这个内核参数用来快速回收 TIME_WAIT 连接，不过如果在 NAT 环境下会引发问题。 RFC1323 中有如下一段描述： An additional mechanism could be added to the TCP, a per-host cache of the last timestamp received from any connection. This value could then be used in the PAWS mechanism to reject old duplicate segments from earlier incarnations of the connection, if the timestamp clock can be guaranteed to have ticked at least once since the old connection was open. This would require that the TIME-WAIT delay plus the RTT together must be at least one tick of the sender’s timestamp clock. Such an extension is not part of the proposal of this RFC. 大概意思是说TCP有一种行为，可以缓存每个连接最新的时间戳，后续请求中如果时间戳小于缓存的时间戳，即视为无效，相应的数据包会被丢弃。 Linux是否启用这种行为取决于tcp_timestamps和tcp_tw_recycle，因为tcp_timestamps缺省就是开启的，所以当tcp_tw_recycle被开启后，实际上这种行为就被激活了，当客户端或服务端以NAT方式构建的时候就可能出现问题，下面以客户端NAT为例来说明： 当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。如果发生了此类问题，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。 在4.12之后的内核已移除tcp_tw_recycle内核参数: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc https://github.com/torvalds/linux/commit/4396e46187ca5070219b81773c4e65088dac50cc © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/arp_cache-neighbor-table-overflow.html":{"url":"troubleshooting/node/arp_cache-neighbor-table-overflow.html","title":"arp_cache: neighbor table overflow!","keywords":"","body":"arp_cache: neighbor table overflow! TODO 优化 现象 内核日志报错: arp_cache: neighbor table overflow! 原因 查看当前 arp 记录数: $ arp -an | wc -l 1335 查看 gc 阀值: $ sysctl -a | grep net.ipv4.neigh.default.gc_thresh net.ipv4.neigh.default.gc_thresh1 = 128 net.ipv4.neigh.default.gc_thresh2 = 512 net.ipv4.neigh.default.gc_thresh3 = 1024 当前 arp 记录数接近 gc_thresh3 比较容易 overflow，因为当 arp 记录达到 gc_thresh3 时会强制触发 gc 清理，当这时又有数据包要发送，并且根据目的 IP 在 arp cache 中没找到 mac 地址，这时会判断当前 arp cache 记录数加 1 是否大于 gc_thresh3，如果没有大于就会 时就会报错: neighbor table overflow! 什么情况下会发生? 集群规模大，node 和 pod 数量超多 案例一 TKE 一用户某集群节点数 1200+，用户监控方案是 daemonset 部署 node-exporter 暴露节点监控指标，使用 hostNework 方式，statefulset 部署 promethues 且仅有一个实例，落在了一个节点上，promethues 请求所有节点 node-exporter 获取节点监控指标，也就是或扫描所有节点，导致 arp cache 需要存所有 node 的记录，而节点数 1200+，大于了 net.ipv4.neigh.default.gc_thresh3 的默认值 1024，这个值是个硬限制，arp cache记录数大于这个就会强制触发 gc，所以会造成频繁gc，当有数据包发送会查本地 arp，如果本地没找到 arp 记录就会判断当前 arp cache 记录数+1是否大于 gc_thresh3，如果没有就会广播 arp 查询 mac 地址，如果大于了就直接报 arp_cache: neighbor table overflow!，并且放弃 arp 请求，无法获取 mac 地址也就无法知道探测报文该往哪儿发(即便就在本机某个 veth pair)，kubelet 对本机 pod 做存活检查发 arp 查 mac 地址，在 arp cahce 找不到，由于这时 arp cache已经满了，刚要 gc 但还没做所以就只有报错丢包，导致存活检查失败重启 pod 解决方案 调整部分节点内核参数，将 arp cache 的 gc 阀值调高 (/etc/sysctl.conf): net.ipv4.neigh.default.gc_thresh1 = 80000 net.ipv4.neigh.default.gc_thresh2 = 90000 net.ipv4.neigh.default.gc_thresh3 = 100000 并给 node 打下label，修改 pod spec，加下 nodeSelector 或者 nodeAffnity，让 pod 只调度到这部分改过内核参数的节点 参考资料 Scaling Kubernetes to 2,500 Nodes: https://openai.com/blog/scaling-kubernetes-to-2500-nodes/ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/node/cannot-allocate-memory.html":{"url":"troubleshooting/node/cannot-allocate-memory.html","title":"Cannot allocate memory","keywords":"","body":"Cannot allocate memory 容器启动失败，报错 Cannot allocate memory PID 满了 如果登录 ssh 困难，并且登录成功后执行任意命名经常报 Cannot allocate memory，多半是 PID 占满了。 看下 PID 限制: cat /proc/sys/kernel/pid_max 也看下是否还有当前用户的 ulimit 限制最大进程数 再看下当前 PID 数量: ps -eLf | wc -l 如果发现 PID 数量解决 limit，可以调大下限制: 临时调大： echo 65535 > /proc/sys/kernel/pid_max 永久调大: echo \"kernel.pid_max=65535 \" >> /etc/sysctl.conf && sysctl -p © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/network/service-unreachable.html":{"url":"troubleshooting/network/service-unreachable.html","title":"Service 访问不通","keywords":"","body":"Service 访问不通 TODO 可能原因： 集群 dns 故障 节点防火墙没放开集群容器网络 (iptables/安全组) © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/network/service-cannot-resolve.html":{"url":"troubleshooting/network/service-cannot-resolve.html","title":"Service 无法解析","keywords":"","body":"Service 无法解析 TODO 优化 检查 dns 服务是否正常(kube-dns或CoreDNS) kubelet 启动参数 --cluster-dns 可以看到 dns 服务的 cluster ip: $ ps -ef | grep kubelet ... /usr/bin/kubelet --cluster-dns=172.16.14.217 ... 找到 dns 的 service: $ kubectl get svc -n kube-system | grep 172.16.14.217 kube-dns ClusterIP 172.16.14.217 53/TCP,53/UDP 47d 看是否存在 endpoint: $ kubectl -n kube-system describe svc kube-dns | grep -i endpoints Endpoints: 172.16.0.156:53,172.16.0.167:53 Endpoints: 172.16.0.156:53,172.16.0.167:53 检查 endpoint 的 对应 pod 是否正常: $ kubectl -n kube-system get pod -o wide | grep 172.16.0.156 kube-dns-898dbbfc6-hvwlr 3/3 Running 0 8d 172.16.0.156 10.0.0.3 dns 服务正常，pod 与 dns 服务之间网络不通 检查 dns 服务运行正常，再检查下 pod 是否连不上 dns 服务，可以在 pod 里 telnet 一下 dns 的 53 端口: # 连 dns service 的 cluster ip $ telnet 172.16.14.217 53 如果检查到是网络不通，就需要排查下网络设置 检查节点的安全组设置，需要放开集群的容器网段 检查是否还有防火墙规则，检查 iptables © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/network/lb-healthcheck-failed.html":{"url":"troubleshooting/network/lb-healthcheck-failed.html","title":"LB 健康检查失败","keywords":"","body":"LB 健康检查失败 TODO 可能原因: 节点防火墙规则没放开 nodeport 区间端口 (默认 30000-32768) 检查iptables和云主机安全组 LB IP 绑到 kube-ipvs0 导致丢源 IP为 LB IP 的包: https://github.com/kubernetes/kubernetes/issues/79783 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"troubleshooting/network/dns-lookup-5s-delay.html":{"url":"troubleshooting/network/dns-lookup-5s-delay.html","title":"DNS 5秒延时","keywords":"","body":"DNS 5 秒延时 延时现象 客户反馈从 pod 中访问服务时，总是有些请求的响应时延会达到5秒。正常的响应只需要毫秒级别的时延。 抓包 通过 nsenter 进入 pod netns，使用节点上的 tcpdump 抓 pod 中的包 (抓包方法参考这里)，发现是有的 DNS 请求没有收到响应，超时 5 秒后，再次发送 DNS 请求才成功收到响应。 在 kube-dns pod 抓包，发现是有 DNS 请求没有到达 kube-dns pod，在中途被丢弃了。 为什么是 5 秒？ man resolv.conf 可以看到 glibc 的 resolver 的缺省超时时间是 5s: timeout:n Sets the amount of time the resolver will wait for a response from a remote name server before retrying the query via a different name server. Measured in seconds, the default is RES_TIMEOUT (currently 5, see ). The value for this option is silently capped to 30. 丢包原因 经过搜索发现这是一个普遍问题。 根本原因是内核 conntrack 模块的 bug，netfilter 做 NAT 时可能发生资源竞争导致部分报文丢弃。 Weave works的工程师 Martynas Pumputis 对这个问题做了很详细的分析：Racy conntrack and DNS lookup timeouts 相关结论： 只有多个线程或进程，并发从同一个 socket 发送相同五元组的 UDP 报文时，才有一定概率会发生 glibc, musl(alpine linux的libc库)都使用 \"parallel query\", 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃 由于 ipvs 也使用了 conntrack, 使用 kube-proxy 的 ipvs 模式，并不能避免这个问题 问题的根本解决 Martynas 向内核提交了两个 patch 来 fix 这个问题，不过他说如果集群中有多个DNS server的情况下，问题并没有完全解决。 其中一个 patch 已经在 2018-7-18 被合并到 linux 内核主线中: netfilter: nf_conntrack: resolve clash for matching conntracks 目前只有4.19.rc 版本包含这个patch。 规避办法 规避方案一：使用TCP发送DNS请求 由于TCP没有这个问题，有人提出可以在容器的resolv.conf中增加options use-vc, 强制glibc使用TCP协议发送DNS query。下面是这个man resolv.conf中关于这个选项的说明： use-vc (since glibc 2.14) Sets RES_USEVC in _res.options. This option forces the use of TCP for DNS resolutions. 笔者使用镜像\"busybox:1.29.3-glibc\" (libc 2.24) 做了试验，并没有见到这样的效果，容器仍然是通过UDP发送DNS请求。 规避方案二：避免相同五元组DNS请求的并发 resolv.conf还有另外两个相关的参数： single-request-reopen (since glibc 2.9) single-request (since glibc 2.10) man resolv.conf中解释如下： single-request-reopen (since glibc 2.9) Sets RES_SNGLKUPREOP in _res.options. The resolver uses the same socket for the A and AAAA requests. Some hardware mistakenly sends back only one reply. When that happens the client system will sit and wait for the second reply. Turning this option on changes this behavior so that if two requests from the same port are not handled correctly it will close the socket and open a new one before sending the second request. single-request (since glibc 2.10) Sets RES_SNGLKUP in _res.options. By default, glibc performs IPv4 and IPv6 lookups in parallel since version 2.9. Some appliance DNS servers cannot handle these queries properly and make the requests time out. This option disables the behavior and makes glibc perform the IPv6 and IPv4 requests sequentially (at the cost of some slowdown of the resolving process). 用自己的话解释下： single-request-reopen: 发送 A 类型请求和 AAAA 类型请求使用不同的源端口，这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突 single-request: 避免并发，改为串行发送 A 类型和 AAAA 类型请求，没有了并发，从而也避免了冲突 要给容器的 resolv.conf 加上 options 参数，有几个办法： 1) 在容器的 \"ENTRYPOINT\" 或者 \"CMD\" 脚本中，执行 /bin/echo 'options single-request-reopen' >> /etc/resolv.conf 2) 在 pod 的 postStart hook 中: lifecycle: postStart: exec: command: - /bin/sh - -c - \"/bin/echo 'options single-request-reopen' >> /etc/resolv.conf\" 3) 使用 template.spec.dnsConfig (k8s v1.9 及以上才支持): template: spec: dnsConfig: options: - name: single-request-reopen 4) 使用 ConfigMap 覆盖 pod 里面的 /etc/resolv.conf: configmap: apiVersion: v1 data: resolv.conf: | nameserver 1.2.3.4 search default.svc.cluster.local svc.cluster.local cluster.local ec2.internal options ndots:5 single-request-reopen timeout:1 kind: ConfigMap metadata: name: resolvconf pod spec: volumeMounts: - name: resolv-conf mountPath: /etc/resolv.conf subPath: resolv.conf ... volumes: - name: resolv-conf configMap: name: resolvconf items: - key: resolv.conf path: resolv.conf 5) 使用 MutatingAdmissionWebhook MutatingAdmissionWebhook 是 1.9 引入的 Controller，用于对一个指定的 Resource 的操作之前，对这个 resource 进行变更。 istio 的自动 sidecar注入就是用这个功能来实现的。 我们也可以通过 MutatingAdmissionWebhook，来自动给所有POD，注入以上3)或者4)所需要的相关内容。 以上方法中， 1)和2)都需要修改镜像， 3)和4)则只需要修改POD的spec， 能适用于所有镜像。不过还是有不方便的地方： 每个工作负载的yaml都要做修改，比较麻烦 对于通过helm创建的工作负载，需要修改helm charts 方法5)对集群使用者最省事，照常提交工作负载即可。不过初期需要一定的开发工作量。 规避方案三：使用本地DNS缓存 容器的DNS请求都发往本地的DNS缓存服务(dnsmasq, nscd等)，不需要走DNAT，也不会发生conntrack冲突。另外还有个好处，就是避免DNS服务成为性能瓶颈。 使用本地DNS缓存有两种方式： 每个容器自带一个DNS缓存服务 每个节点运行一个DNS缓存服务，所有容器都把本节点的DNS缓存作为自己的 nameserver 从资源效率的角度来考虑的话，推荐后一种方式。官方也意识到了这个问题比较常见，给出了 coredns 以 cache 模式作为 daemonset 部署的解决方案: https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/ 实施办法 条条大路通罗马，不管怎么做，最终到达上面描述的效果即可。 POD中要访问节点上的DNS缓存服务，可以使用节点的IP。 如果节点上的容器都连在一个虚拟bridge上， 也可以使用这个bridge的三层接口的IP(在TKE中，这个三层接口叫cbr0)。 要确保DNS缓存服务监听这个地址。 如何把POD的/etc/resolv.conf中的nameserver设置为节点IP呢？ 一个办法，是设置 POD.spec.dnsPolicy 为 \"Default\"， 意思是POD里面的 /etc/resolv.conf， 使用节点上的文件。缺省使用节点上的 /etc/resolv.conf(如果kubelet通过参数--resolv-conf指定了其他文件，则使用--resolv-conf所指定的文件)。 另一个办法，是给每个节点的kubelet指定不同的--cluster-dns参数，设置为节点的IP，POD.spec.dnsPolicy仍然使用缺省值\"ClusterFirst\"。 kops项目甚至有个issue在讨论如何在部署集群时设置好--cluster-dns指向节点IP: https://github.com/kubernetes/kops/issues/5584 参考资料 Racy conntrack and DNS lookup timeouts: https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts 5 – 15s DNS lookups on Kubernetes? : https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/ DNS intermittent delays of 5s: https://github.com/kubernetes/kubernetes/issues/56903 记一次Docker/Kubernetes上无法解释的连接超时原因探寻之旅: https://mp.weixin.qq.com/s/VYBs8iqf0HsNg9WAxktzYQ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/cluster-permission-control.html":{"url":"best-practice/cluster-permission-control.html","title":"集群权限控制","keywords":"","body":"集群权限控制 简单粗暴的最高权限 为了简单方便，通常我们使用 token 认证，在 kubeconfig 中配置的账号拥有集群最高权限的 token，可以做任何操作。 开启 token 认证的方法: kube-apiserver 启动参数 --token-auth-file 传一个 token 认证文件，比如: --token-auth-file=/etc/kubernetes/known_tokens.csv token 认证文件包含最高权限 token，比如: wJmqmTMK7BMNOfC1YDmOVydboBdOPPWj,admin,admin,system:masters 在 kubeconfig (~/.kube/config) 中配置 token: apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURaRENDQWt5Z0F3SUJBZ0lJQVhqdnpFVDBMcFF3RFFZSktvWklodmNOQVFFTEJRQXdVREVMTUFrR0ExVUUKQmhNQ1EwNHhLakFSQmdOVkJBb1RDblJsYm1ObGJuUjVkVzR3RlFZRFZRUUtFdzV6ZVhOMFpXMDZiV0Z6ZEdWeQpjekVWTUJNR0ExVUVBeE1NWTJ4ekxXRjJPVFZxZEdJeU1CNFhEVEU1TURReE9UQXpNRFl4TWxvWERUTTVNRFF4Ck9UQXpNRFl4TWxvd1VERUxNQWtHQTFVRUJoTUNRMDR4S2pBUkJnTlZCQW9UQ25SbGJtTmxiblI1ZFc0d0ZRWUQKVlFRS0V3NXplWE4wWlcwNmJXRnpkR1Z5Y3pFVk1CTUdBMVVFQXhNTVkyeHpMV0YyT1RWcWRHSXlNSUlCSWpBTgpCZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF5bHRhNytQSVhTUk45ZUtrMUtCRG9hWjFRZ0YxCjBORG5tWFl5V3BTREZvb3JJN2V6eFVTQzNydVFiWk5MYXM0cDJTRE02U0ZyVEkzRHI4dUZETytUV2k0aFFQNTYKak1zUHBSTUdCZ29hNzV0bkRTanY4TkgwUitFak0vdmNxQ3hWc1hZeUFSZEZlVDZ0dEFNZU9IcGRpYk5yTEN3dgpPSzBBVnl4OFlpeHI2bFpSQ1BKMTEwcmlPVllGNlgzMVZhUmNMTmJ5d1lJWGdiWUdVTC9UZEZoUExGUDRpTU5BCnRtaWYzUG9tUnZUcDI3R3RFcUlicndVbUdNT3hGV25LTWo0dXlIcTlZS0phYjVlRGV3V1liZ2craW9HUVJwcG0KSVJXdUQ5RFVOaHVRL3RKWVBodVJ0VzY5c2FzVVRpSjNmaThDQmhSN1ZyVDZ1Z0ZKc2J3RlZYN3d2d0lEQVFBQgpvMEl3UURBT0JnTlZIUThCQWY4RUJBTUNBb1F3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdJR0NDc0dBUVVGCkJ3TUJNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSXZxT1V0SGU1Zy8KdDJsMWM2UEFkdlgrWkRPdE04S0JyZEdZd2RQQVplSTU3WklOT2p6ZFprWG9hTmY0aXZCekxab1pYSmZ4b1NWLwoyVEQrSUM4TFN1S1JvMlh0Z1Z1WnRVb3htUitMYXdmUjZvSmFDT0xKRmdVemdlaTcwTHJiTWI1cUkrMUV1TnBaCkt0TTdRQmtDSG5UdFZzbGM0czJpeTZvMFJFSGpady9NV04xanE3V1QxYVpKUGMydlYxWmlReUpFM0xNT21QYksKSzhtdFBnZUxBcTN0KzRUanRkWGY4TEJBb3dxZDNLakpiMGF2QXNaSFpGNEg0azI1d0VneFFIaDNmUnU5eEdudgpPemt4eUd0NWh3RWg0QXVhcVMyRUlMRUxWTmlydmFKSzEzY2EwNldQSmdYNUlqWnlnNUtqbGxPU2RZWlpsYnR1CloydFRheXA3b3djPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== server: https://169.254.128.15:60002 name: local contexts: - context: cluster: local user: admin name: master current-context: master kind: Config preferences: {} users: - name: admin user: token: wJmqmTMK7BMNOfC1YDmOVydboBdOPPWj 使用 RBAC 细化用户权限 如果可以操作集群的人比较多，使用最高权限的 token 可能会比较危险，如果有人误操作或恶意操作，即使 apiserver 开启审计也无法知道是谁做的操作，所以最好控制下权限，分发不同的 kubeconfig 给不同的人，这个可以通过 RBAC 来实现(确保 kube-apiserver 启动参数 --authorization-mode 包含 RBAC)，基本思想是创建 ServiceAccount 绑定 Role 或 ClusterRole 来控制权限，拿到给 ServiceAccount 自动创建的 secret 中的 token 后 base64 解码再配置到 kubeconfig 中。 RBAC 示例 给 roc 授权 test 命名空间所有权限，istio-system 命名空间的只读权限: apiVersion: v1 kind: ServiceAccount metadata: name: roc namespace: default --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: admin namespace: test rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"*\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: admin-to-roc namespace: test subjects: - kind: ServiceAccount name: roc namespace: default roleRef: kind: Role name: admin apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: readonly namespace: istio-system rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"get\", \"list\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: readonly-to-roc subjects: - kind: ServiceAccount name: roc namespace: default roleRef: kind: Role name: istio-system apiGroup: rbac.authorization.k8s.io 给 roc 授权整个集群的只读权限: apiVersion: v1 kind: ServiceAccount metadata: name: roc namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: readonly rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"get\", \"list\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: readonly-to-roc subjects: - kind: ServiceAccount name: roc namespace: kube-system roleRef: kind: ClusterRole name: readonly apiGroup: rbac.authorization.k8s.io 有几点说明下: ServiceAccount 本身在哪个命名空间并不重要 权限控制细节在于 Role 和 ClusterRole 的 rules 获取 Token 创建好了 ServiceAccount，我们来获取下它的 token: $ kubectl get serviceaccount roc -n kube-system -o jsonpath='{.secrets[0].name}' | xargs kubectl get secret -n kube-system -o jsonpath='{.data.token}' | base64 -d eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyb2MtdG9rZW4teGR0ZzkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicm9jIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTAyMDM1M2YtOGQ4MC0xMWU5LTkyNDUtZDJhN2YzOWY0ODhkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJvYyJ9.CQorLS_DZVsMuMNRCU39bob5l2PpgFk9ribeRxpqJTmEMbdDlmax66RXOVbBzFhPiPIrZ2xLySOkkFzTXiijHVTpF4v80FlmvNQoCTDKN8-FQ8132QKieATwAeQu01e2uPYo8f9Gb1ymoJbLVTqMNtzX-dij0bpVwxsk1SvdeyqEuSIjKsTwaUBxNml9X4Ba-fdaDf6jKmXONAGy3K89GqFkl3Aabxyc1eG4aCuJRaGBVeMTgZnp2yzhVpwXZkBPcw8wGhjonWr3xZp-iReXra-Ko2mqTQaoEZb87HHq43gF1lZGFng6xyoXKoQ0j_wx6p_T5U85hA-ZnrpSnR5K2Q 替换 kube-system 为 ServiceAccount 所在的命名空间 替换 roc 为 ServiceAccount 的名称 配置 kubeconfig 将 token 配到 kubeconfig 中: apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURaRENDQWt5Z0F3SUJBZ0lJQVhqdnpFVDBMcFF3RFFZSktvWklodmNOQVFFTEJRQXdVREVMTUFrR0ExVUUKQmhNQ1EwNHhLakFSQmdOVkJBb1RDblJsYm1ObGJuUjVkVzR3RlFZRFZRUUtFdzV6ZVhOMFpXMDZiV0Z6ZEdWeQpjekVWTUJNR0ExVUVBeE1NWTJ4ekxXRjJPVFZxZEdJeU1CNFhEVEU1TURReE9UQXpNRFl4TWxvWERUTTVNRFF4Ck9UQXpNRFl4TWxvd1VERUxNQWtHQTFVRUJoTUNRMDR4S2pBUkJnTlZCQW9UQ25SbGJtTmxiblI1ZFc0d0ZRWUQKVlFRS0V3NXplWE4wWlcwNmJXRnpkR1Z5Y3pFVk1CTUdBMVVFQXhNTVkyeHpMV0YyT1RWcWRHSXlNSUlCSWpBTgpCZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF5bHRhNytQSVhTUk45ZUtrMUtCRG9hWjFRZ0YxCjBORG5tWFl5V3BTREZvb3JJN2V6eFVTQzNydVFiWk5MYXM0cDJTRE02U0ZyVEkzRHI4dUZETytUV2k0aFFQNTYKak1zUHBSTUdCZ29hNzV0bkRTanY4TkgwUitFak0vdmNxQ3hWc1hZeUFSZEZlVDZ0dEFNZU9IcGRpYk5yTEN3dgpPSzBBVnl4OFlpeHI2bFpSQ1BKMTEwcmlPVllGNlgzMVZhUmNMTmJ5d1lJWGdiWUdVTC9UZEZoUExGUDRpTU5BCnRtaWYzUG9tUnZUcDI3R3RFcUlicndVbUdNT3hGV25LTWo0dXlIcTlZS0phYjVlRGV3V1liZ2craW9HUVJwcG0KSVJXdUQ5RFVOaHVRL3RKWVBodVJ0VzY5c2FzVVRpSjNmaThDQmhSN1ZyVDZ1Z0ZKc2J3RlZYN3d2d0lEQVFBQgpvMEl3UURBT0JnTlZIUThCQWY4RUJBTUNBb1F3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdJR0NDc0dBUVVGCkJ3TUJNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSXZxT1V0SGU1Zy8KdDJsMWM2UEFkdlgrWkRPdE04S0JyZEdZd2RQQVplSTU3WklOT2p6ZFprWG9hTmY0aXZCekxab1pYSmZ4b1NWLwoyVEQrSUM4TFN1S1JvMlh0Z1Z1WnRVb3htUitMYXdmUjZvSmFDT0xKRmdVemdlaTcwTHJiTWI1cUkrMUV1TnBaCkt0TTdRQmtDSG5UdFZzbGM0czJpeTZvMFJFSGpady9NV04xanE3V1QxYVpKUGMydlYxWmlReUpFM0xNT21QYksKSzhtdFBnZUxBcTN0KzRUanRkWGY4TEJBb3dxZDNLakpiMGF2QXNaSFpGNEg0azI1d0VneFFIaDNmUnU5eEdudgpPemt4eUd0NWh3RWg0QXVhcVMyRUlMRUxWTmlydmFKSzEzY2EwNldQSmdYNUlqWnlnNUtqbGxPU2RZWlpsYnR1CloydFRheXA3b3djPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== server: https://169.254.128.15:60002 name: local contexts: - context: cluster: local user: admin name: master current-context: master kind: Config preferences: {} users: - name: roc user: token: 'eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyb2MtdG9rZW4teGR0ZzkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicm9jIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTAyMDM1M2YtOGQ4MC0xMWU5LTkyNDUtZDJhN2YzOWY0ODhkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJvYyJ9.CQorLS_DZVsMuMNRCU39bob5l2PpgFk9ribeRxpqJTmEMbdDlmax66RXOVbBzFhPiPIrZ2xLySOkkFzTXiijHVTpF4v80FlmvNQoCTDKN8-FQ8132QKieATwAeQu01e2uPYo8f9Gb1ymoJbLVTqMNtzX-dij0bpVwxsk1SvdeyqEuSIjKsTwaUBxNml9X4Ba-fdaDf6jKmXONAGy3K89GqFkl3Aabxyc1eG4aCuJRaGBVeMTgZnp2yzhVpwXZkBPcw8wGhjonWr3xZp-iReXra-Ko2mqTQaoEZb87HHq43gF1lZGFng6xyoXKoQ0j_wx6p_T5U85hA-ZnrpSnR5K2Q' 或者用 kubectl 生成 kubeconfig: # 配置user entry kubectl config set-credentials --token='' # 配置cluster entry kubectl config set-cluster --server= --certificate-authority= # 配置context entry kubectl config set-context --cluster= --user= # 查看 kubectl config view # 配置当前使用的context kubectl config use-context © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/kubernetes-grace-update.html":{"url":"best-practice/kubernetes-grace-update.html","title":"优雅热更新","keywords":"","body":"优雅热更新 当kubernetes对服务滚动更新的期间，默认配置的情况下可能会让部分连接异常（比如连接被拒绝），我们来分析下原因并给出最佳实践 滚动更新场景 使用 deployment 部署服务并关联 service 修改 deployment 的 replica 调整副本数量来滚动更新 升级程序版本(修改镜像tag)触发 deployment 新建 replicaset 启动新版本的 pod 使用 HPA (HorizontalPodAutoscaler) 来对 deployment 自动扩缩容 更新过程连接异常的原因 滚动更新时，service 对应的 pod 会被创建或销毁，也就是 service 对应的 endpoint 列表会新增或移除endpoint，更新期间可能让部分连接异常，主要原因是： pod 被创建，还没完全启动就被 endpoint controller 加入到 service 的 endpoint 列表，然后 kube-proxy 配置对应的路由规则(iptables/ipvs)，如果请求被路由到还没完全启动完成的 pod，这时 pod 还不能正常处理请求，就会导致连接异常 pod 被销毁，但是从 endpoint controller watch 到变化并更新 service 的 endpoint 列表到 kube-proxy 更新路由规则这期间有个时间差，pod可能已经完全被销毁了，但是路由规则还没来得及更新，造成请求依旧还能被转发到已经销毁的 pod ip，导致连接异常 最佳实践 针对第一种情况，可以给 pod 里的 container 加 readinessProbe (就绪检查)，这样可以让容器完全启动了才被endpoint controller加进 service 的 endpoint 列表，然后 kube-proxy 再更新路由规则，这时请求被转发到的所有后端 pod 都是正常运行，避免了连接异常 针对第二种情况，可以给 pod 里的 container 加 preStop hook，让 pod 真正销毁前先 sleep 等待一段时间，留点时间给 endpoint controller 和 kube-proxy 清理 endpoint 和路由规则，这段时间 pod 处于 Terminating 状态，在路由规则更新完全之前如果有请求转发到这个被销毁的 pod，请求依然可以被正常处理，因为它还没有被真正销毁 最佳实践 yaml 示例: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: component: nginx template: metadata: labels: component: nginx spec: containers: - name: nginx image: \"nginx\" ports: - name: http hostPort: 80 containerPort: 80 protocol: TCP readinessProbe: httpGet: path: /healthz port: 80 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 15 timeoutSeconds: 1 lifecycle: preStop: exec: command: [\"/bin/bash\", \"-c\", \"sleep 30\"] 参考资料 Container probes: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes Container Lifecycle Hooks: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/scale-keepalive-service.html":{"url":"best-practice/scale-keepalive-service.html","title":"解决长连接服务扩容失效","keywords":"","body":"解决长连接服务扩容失效 在现网运营中，有很多场景为了提高效率，一般都采用建立长连接的方式来请求。我们发现在客户端以长连接请求服务端的场景下，K8S的自动扩容会失效。原因是客户端长连接一直保留在老的Pod容器中，新扩容的Pod没有新的连接过来，导致K8S按照步长扩容第一批Pod之后就停止了扩容操作，而且新扩容的Pod没能承载请求，进而出现服务过载的情况，自动扩容失去了意义。 对长连接扩容失效的问题，我们的解决方法是将长连接转换为短连接。我们参考了 nginx keepalive 的设计，nginx 中 keepalive_requests 这个配置项设定了一个TCP连接能处理的最大请求数，达到设定值(比如1000)之后服务端会在 http 的 Header 头标记 “Connection:close”，通知客户端处理完当前的请求后关闭连接，新的请求需要重新建立TCP连接，所以这个过程中不会出现请求失败，同时又达到了将长连接按需转换为短连接的目的。通过这个办法客户端和云K8S服务端处理完一批请求后不断的更新TCP连接，自动扩容的新Pod能接收到新的连接请求，从而解决了自动扩容失效的问题。 由于Golang并没有提供方法可以获取到每个连接处理过的请求数，我们重写了 net.Listener 和 net.Conn，注入请求计数器，对每个连接处理的请求做计数，并通过 net.Conn.LocalAddr() 获得计数值，判断达到阈值 1000 后在返回的 Header 中插入 “Connection:close” 通知客户端关闭连接，重新建立连接来发起请求。以上处理逻辑用 Golang 实现示例代码如下： package main import ( \"net\" \"github.com/gin-gonic/gin\" \"net/http\" ) //重新定义net.Listener type counterListener struct { net.Listener } //重写net.Listener.Accept(),对接收到的连接注入请求计数器 func (c *counterListener) Accept() (net.Conn, error) { conn, err := c.Listener.Accept() if err != nil { return nil, err } return &counterConn{Conn: conn}, nil } //定义计数器counter和计数方法Increment() type counter int func (c *counter) Increment() int { *c++ return int(*c) } //重新定义net.Conn,注入计数器ct type counterConn struct { net.Conn ct counter } //重写net.Conn.LocalAddr()，返回本地网络地址的同时返回该连接累计处理过的请求数 func (c *counterConn) LocalAddr() net.Addr { return &counterAddr{c.Conn.LocalAddr(), &c.ct} } //定义TCP连接计数器,指向连接累计请求的计数器 type counterAddr struct { net.Addr *counter } func main() { r := gin.New() r.Use(func(c *gin.Context) { localAddr := c.Request.Context().Value(http.LocalAddrContextKey) if ct, ok := localAddr.(interface{ Increment() int }); ok { if ct.Increment() >= 1000 { c.Header(\"Connection\", \"close\") } } c.Next() }) r.GET(\"/\", func(c *gin.Context) { c.String(200, \"plain/text\", \"hello\") }) l, err := net.Listen(\"tcp\", \":8080\") if err != nil { panic(err) } err = http.Serve(&counterListener{l}, r) if err != nil { panic(err) } } © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/handle-cgroup-oom-with-oom-guard-in-userspace.html":{"url":"best-practice/handle-cgroup-oom-with-oom-guard-in-userspace.html","title":"使用 oom-guard 在用户态处理 cgroup OOM","keywords":"","body":"使用 oom-guard 在用户态处理 cgroup OOM 背景 由于 linux 内核对 cgroup OOM 的处理，存在很多 bug，经常有由于频繁 cgroup OOM 导致节点故障(卡死， 重启， 进程异常但无法杀死)，于是 TKE 团队开发了 oom-guard，在用户态处理 cgroup OOM 规避了内核 bug。 原理 核心思想是在发生内核 cgroup OOM kill 之前，在用户空间杀掉超限的容器， 减少走到内核 cgroup 内存回收失败后的代码分支从而触发各种内核故障的机会。 threshold notify 参考文档: https://lwn.net/Articles/529927/ oom-guard 会给 memory cgroup 设置 threshold notify， 接受内核的通知。 以一个例子来说明阀值计算通知原理: 一个 pod 设置的 memory limit 是 1000M， oom-guard 会根据配置参数计算出 margin: margin = 1000M * margin_ratio = 20M // 缺省margin_ratio是0.02 margin 最小不小于 mim_margin(缺省1M)， 最大不大于 max_margin(缺省为30M)。如果超出范围，则取 mim_margin 或 max_margin。计算 threshold = limit - margin ，也就是 1000M - 20M = 980M，把 980M 作为阈值设置给内核。当这个 pod 的内存使用量达到 980M 时， oom-guard 会收到内核的通知。 在触发阈值之前，oom-gurad 会先通过 memory.force_empty 触发相关 cgroup 的内存回收。 另外，如果触发阈值时，相关 cgroup 的 memory.stat 显示还有较多 cache， 则不会触发后续处理策略，这样当 cgroup 内存达到 limit 时，会内核会触发内存回收。 这个策略也会造成部分容器内存增长太快时，还是会触发内核 cgroup OOM 达到阈值后的处理策略 通过 --policy 参数来控制处理策略。目前有三个策略， 缺省策略是 process。 process: 采用跟内核cgroup OOM killer相同的策略，在该cgroup内部，选择一个 oom_score 得分最高的进程杀掉。 通过 oom-guard 发送 SIGKILL 来杀掉进程 container: 在该cgroup下选择一个 docker 容器，杀掉整个容器 noop: 只记录日志，并不采取任何措施 事件上报 通过 webhook reporter 上报 k8s event，便于分析统计，使用kubectl get event 可以看到: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 14s 14s 1 172.21.16.23.158b732d352bcc31 Node Warning OomGuardKillContainer oom-guard, 172.21.16.23 {\"hostname\":\"172.21.16.23\",\"timestamp\":\"2019-03-13T07:12:14.561650646Z\",\"oomcgroup\":\"/sys/fs/cgroup/memory/kubepods/burstable/pod3d6329e5-455f-11e9-a7e5-06925242d7ea/223d4795cc3b33e28e702f72e0497e1153c4a809de6b4363f27acc12a6781cdb\",\"proccgroup\":\"/sys/fs/cgroup/memory/kubepods/burstable/pod3d6329e5-455f-11e9-a7e5-06925242d7ea/223d4795cc3b33e28e702f72e0497e1153c4a809de6b4363f27acc12a6781cdb\",\"threshold\":205520896,\"usage\":206483456,\"killed\":\"16481(fakeOOM) \",\"stats\":\"cache 20480|rss 205938688|rss_huge 199229440|mapped_file 0|dirty 0|writeback 0|pgpgin 1842|pgpgout 104|pgfault 2059|pgmajfault 0|inactive_anon 8192|active_anon 203816960|inactive_file 0|active_file 0|unevictable 0|hierarchical_memory_limit 209715200|total_cache 20480|total_rss 205938688|total_rss_huge 199229440|total_mapped_file 0|total_dirty 0|total_writeback 0|total_pgpgin 1842|total_pgpgout 104|total_pgfault 2059|total_pgmajfault 0|total_inactive_anon 8192|total_active_anon 203816960|total_inactive_file 0|total_active_file 0|total_unevictable 0|\",\"policy\":\"Container\"} 使用方法 部署 保存部署 yaml: oom-guard.yaml: apiVersion: v1 kind: ServiceAccount metadata: name: oomguard namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:oomguard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: oomguard namespace: kube-system --- apiVersion: apps/v1 kind: DaemonSet metadata: name: oom-guard namespace: kube-system labels: app: oom-guard spec: selector: matchLabels: app: oom-guard template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: app: oom-guard spec: serviceAccountName: oomguard hostPID: true hostNetwork: true dnsPolicy: ClusterFirst containers: - name: k8s-event-writer image: ccr.ccs.tencentyun.com/paas/k8s-event-writer:v1.6 resources: limits: cpu: 10m memory: 60Mi requests: cpu: 10m memory: 30Mi args: - --logtostderr - --unix-socket=true env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: status.hostIP volumeMounts: - name: unix mountPath: /unix - name: oomguard image: ccr.ccs.tencentyun.com/paas/oomguard:nosoft-v2 imagePullPolicy: Always securityContext: privileged: true resources: limits: cpu: 10m memory: 60Mi requests: cpu: 10m memory: 30Mi volumeMounts: - name: cgroupdir mountPath: /sys/fs/cgroup/memory - name: unix mountPath: /unix - name: kmsg mountPath: /dev/kmsg readOnly: true command: [\"/oom-guard\"] args: - --v=2 - --logtostderr - --root=/sys/fs/cgroup/memory - --walkIntervalSeconds=277 - --inotifyResetSeconds=701 - --port=0 - --margin-ratio=0.02 - --min-margin=1 - --max-margin=30 - --guard-ms=50 - --policy=container - --openSoftLimit=false - --webhook-url=http://localhost/message env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: status.hostIP volumes: - name: cgroupdir hostPath: path: /sys/fs/cgroup/memory - name: unix emptyDir: {} - name: kmsg hostPath: path: /dev/kmsg 一键部署: kubectl apply -f oom-guard.yaml 检查是否部署成功： $ kubectl -n kube-system get ds oom-guard NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE oom-guard 2 2 2 2 2 6m 其中 AVAILABLE 数量跟节点数一致，说明所有节点都已经成功运行了 oom-guard。 查看 oom-guard 日志 kubectl -n kube-system logs oom-guard-xxxxx oomguard 查看 oom 相关事件 kubectl get events |grep CgroupOOM kubectl get events |grep SystemOOM kubectl get events |grep OomGuardKillContainer kubectl get events |grep OomGuardKillProcess 卸载 kubectl delete -f oom-guard.yaml 这个操作可能有点慢，如果一直不返回 (有节点 NotReady 时可能会卡住)，ctrl+C 终止，然后执行下面的脚本: for pod in `kubectl get pod -n kube-system | grep oom-guard | awk '{print $1}'` do kubectl delete pod $pod -n kube-system --grace-period=0 --force done 检查删除操作是否成功 kubectl -n kube-system get ds oom-guard 提示 ...not found 就说明删除成功了 关于开源 当前 oom-gaurd 暂未开源，正在做大量生产试验，后面大量反馈效果统计比较好的时候会考虑开源出来。 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/wildcard-domain-forward.html":{"url":"best-practice/wildcard-domain-forward.html","title":"泛域名动态 Service 转发解决方案","keywords":"","body":"泛域名动态 Service 转发解决方案 需求 集群对外暴露了一个公网IP作为流量入口(可以是 Ingress 或 Service)，DNS 解析配置了一个泛域名指向该IP（比如 *.test.imroc.io），现希望根据请求中不同 Host 转发到不同的后端 Service。比如 a.test.imroc.io 的请求被转发到 my-svc-a，b.test.imroc.io 的请求转发到 my-svc-b。当前 K8S 的 Ingress 并不原生支持这种泛域名转发规则，本文将给出一个解决方案来实现泛域名转发。 简单做法 先说一种简单的方法，这也是大多数人的第一反应：配置 Ingress 规则 假如泛域名有两个不同 Host 分别转发到不同 Service，Ingress 类似这样写: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-ingress spec: rules: - host: a.test.imroc.io http: paths: - backend: serviceName: my-svc-a servicePort: 80 path: / - host: b.test.imroc.io http: paths: - backend: serviceName: my-svc-b servicePort: 80 path: / 但是！如果 Host 非常多会怎样？（比如200+） 每次新增 Host 都要改 Ingress 规则，太麻烦 单个 Ingress 上面的规则越来越多，更改规则对 LB 的压力变大，可能会导致偶尔访问不了 正确姿势 我们可以约定请求中泛域名 Host 通配符的 * 号匹配到的字符跟 Service 的名字相关联（可以是相等，或者 Service 统一在前面加个前缀，比如 a.test.imroc.io 转发到 my-svc-a 这个 Service)，集群内起一个反向代理服务，匹配泛域名的请求全部转发到这个代理服务上，这个代理服务只做一件简单的事，解析 Host，正则匹配抓取泛域名中 * 号这部分，把它转换为 Service 名字，然后在集群里转发（集群 DNS 解析) 这个反向代理服务可以是 Nginx+Lua脚本 来实现，或者自己写个简单程序来做反向代理，这里我用 OpenResty 来实现，它可以看成是 Nginx 的发行版，自带 lua 支持。 有几点需要说明下： 我们使用 nginx 的 proxy_pass 来反向代理到后端服务，proxy_pass 后面跟的变量，我们需要用 lua 来判断 Host 修改变量 nginx 的 proxy_pass 后面跟的如果是可变的域名（非IP，需要 dns 解析)，它需要一个域名解析器，不会走默认的 dns 解析，需要在 nginx.conf 里添加 resolver 配置项来设置一个外部的 dns 解析器 这个解析器我们是用 go-dnsmasq 来实现，它可以将集群的 dns 解析代理给 nginx，以 sidecar 的形式注入到 pod 中，监听 53 端口 nginx.conf 里关键的配置如下图所示： 下面给出完整的 yaml 示例 proxy.yaml: apiVersion: apps/v1beta1 kind: Deployment metadata: labels: component: nginx name: proxy spec: replicas: 1 selector: matchLabels: component: nginx template: metadata: labels: component: nginx spec: containers: - name: nginx image: \"openresty/openresty:centos\" ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - mountPath: /usr/local/openresty/nginx/conf/nginx.conf name: config subPath: nginx.conf - name: dnsmasq image: \"janeczku/go-dnsmasq:release-1.0.7\" args: - --listen - \"127.0.0.1:53\" - --default-resolver - --append-search-domains - --hostsfile=/etc/hosts - --verbose volumes: - name: config configMap: name: configmap-nginx --- apiVersion: v1 kind: ConfigMap metadata: labels: component: nginx name: configmap-nginx data: nginx.conf: |- worker_processes 1; error_log /error.log; events { accept_mutex on; multi_accept on; use epoll; worker_connections 1024; } http { include mime.types; default_type application/octet-stream; log_format main '$time_local $remote_user $remote_addr $host $request_uri $request_method $http_cookie ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\" ' '$request_time $upstream_response_time \"$upstream_cache_status\"'; log_format browser '$time_iso8601 $cookie_km_uid $remote_addr $host $request_uri $request_method ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\" ' '$request_time $upstream_response_time \"$upstream_cache_status\" $http_x_requested_with $http_x_real_ip $upstream_addr $request_body'; log_format client '{\"@timestamp\":\"$time_iso8601\",' '\"time_local\":\"$time_local\",' '\"remote_user\":\"$remote_user\",' '\"http_x_forwarded_for\":\"$http_x_forwarded_for\",' '\"host\":\"$server_addr\",' '\"remote_addr\":\"$remote_addr\",' '\"http_x_real_ip\":\"$http_x_real_ip\",' '\"body_bytes_sent\":$body_bytes_sent,' '\"request_time\":$request_time,' '\"status\":$status,' '\"upstream_response_time\":\"$upstream_response_time\",' '\"upstream_response_status\":\"$upstream_status\",' '\"request\":\"$request\",' '\"http_referer\":\"$http_referer\",' '\"http_user_agent\":\"$http_user_agent\"}'; access_log /access.log main; sendfile on; keepalive_timeout 120s 100s; keepalive_requests 500; send_timeout 60000s; client_header_buffer_size 4k; proxy_ignore_client_abort on; proxy_buffers 16 32k; proxy_buffer_size 64k; proxy_busy_buffers_size 64k; proxy_send_timeout 60000; proxy_read_timeout 60000; proxy_connect_timeout 60000; proxy_cache_valid 200 304 2h; proxy_cache_valid 500 404 2s; proxy_cache_key $host$request_uri$cookie_user; proxy_cache_methods GET HEAD POST; proxy_redirect off; proxy_http_version 1.1; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Frame-Options SAMEORIGIN; server_tokens off; client_max_body_size 50G; add_header X-Cache $upstream_cache_status; autoindex off; resolver 127.0.0.1:53 ipv6=off; server { listen 80; location / { set $service ''; rewrite_by_lua ' local host = ngx.var.host local m = ngx.re.match(host, \"(.+).test.imroc.io\") if m then ngx.var.service = \"my-svc-\" .. m[1] end '; proxy_pass http://$service; } } } 让该代理服务暴露公网访问可以用 Service 或 Ingress 用 Service 的示例 (service.yaml): apiVersion: v1 kind: Service metadata: labels: component: nginx name: service-nginx spec: type: LoadBalancer ports: - name: http port: 80 targetPort: http selector: component: nginx 用 Ingress 的示例 (ingress.yaml): apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-nginx spec: rules: - host: \"*.test.imroc.io\" http: paths: - backend: serviceName: service-nginx servicePort: 80 path: / © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/efficient-kubectl.html":{"url":"best-practice/efficient-kubectl.html","title":"kubectl 高效技巧","keywords":"","body":"kubectl 高效技巧 是否有过因为使用 kubectl 经常需要重复输入命名空间而苦恼？是否觉得应该要有个记住命名空间的功能，自动记住上次使用的命名空间，不需要每次都输入？可惜没有这种功能，但是，本文会教你一个非常巧妙的方法完美帮你解决这个痛点。 k 命令 将如下脚本粘贴到当前shell(注册k命令到当前终端session): function k() { cmdline=`HISTTIMEFORMAT=\"\" history | awk '$2 == \"kubectl\" && (/-n/ || /--namespace/) {for(i=2;i mac 用户可以使用 dash 的 snippets 功能快速将上面的函数粘贴，使用 kk. 作为触发键 (dash snippets可以全局监听键盘输入，使用指定的输入作为触发而展开配置的内容，相当于是全局代码片段)，以后在某个终端想使用 k 的时候按下 kk. 就可以将 k 命令注册到当前终端，dash snippets 配置如图所示： 将 k 当作 kubectl 来用，只是不需要输入命名空间，它会调用 kubectl 并自动加上上次使用的非默认的命名空间，如果想切换命名空间，再常规的使用一次 kubectl 就行，下面是示范： 哈哈，是否感觉可以少输入很多字符，提高 kubectl 使用效率了？这是目前我探索解决 kubectl 重复输入命名空间的最好方案，一开始是受 fuck命令 的启发，想用 go 语言开发个 k 命令，但是发现两个缺点： 需要安装二进制才可以使用（对于需要在多个地方用kubectl管理多个集群的人来说实在太麻烦） 如果当前 shell 默认没有将历史输入记录到 history 文件( bash 的 history 文件默认是 ~/.bash_history)，那么将无法准确知道上一次 kubectl 使用的哪个命名空间 这里解释下第二个缺点的原因：ssh 连上服务器会启动一个 shell 进程，通常是 bash，大多 bash 默认配置会实时将历史输入追加到 ~/.bash_history里，所以开多个ssh使用history命令看到的历史输入是一样的，但有些默认不会实时记录历史到~/.bash_history，而是记在当前 shell 进程的内存中，在 shell 退出时才会写入到文件。这种情况新起的进程是无法知道当前 shell 的最近历史输入的，fuck命令 也不例外。 所以最完美的解决方案就是注册函数到当前shell来调用，配合 dash 的 snippets 功能可以实现快速注册，解决复制粘贴的麻烦 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"best-practice/handle-disk-full.html":{"url":"best-practice/handle-disk-full.html","title":"TODO:处理容器磁盘被写满","keywords":"","body":"处理容器数据磁盘被写满 TODO © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"configuration-management/helm/install-helm.html":{"url":"configuration-management/helm/install-helm.html","title":"安装 Helm","keywords":"","body":"安装 Helm Helm 是 Kubernetes 的包管理器，可以帮我们简化 kubernetes 的操作，一键部署应用。假如你的机器上已经安装了 kubectl 并且能够操作集群，那么你就可以安装 Helm 了。当前最新稳定版是 V2，Helm V3 还未正式发布，下面分别说下安装方法。 安装 Helm V2 执行脚本安装 helm 客户端: $ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 6737 100 6737 0 0 12491 0 --:--:-- --:--:-- --:--:-- 12475 Downloading https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz Preparing to install into /usr/local/bin helm installed into /usr/local/bin/helm Run 'helm init' to configure helm. 查看客户端版本： $ helm version Client: &version.Version{SemVer:\"v2.9.1\", GitCommit:\"20adb27c7c5868466912eebdf6664e7390ebe710\", GitTreeState:\"clean\"} 安装 tiller 服务端到 kubernetes 集群： $ helm init Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! 查看 tiller 是否启动成功: $ kubectl get pods --namespace=kube-system | grep tiller tiller-deploy-dccdb6fd9-2df4r 0/1 ImagePullBackOff 0 14h 如果状态是 ImagePullBackOff ，说明是镜像问题，一般是未拉取到镜像（国内机器拉取不到 gcr.io 下的镜像) 可以查看下是什么镜像: $ kubectl describe pod tiller-deploy-dccdb6fd9-2df4r --namespace=kube-system Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Failed 36m (x5 over 12h) kubelet, k8s-node1 Failed to pull image \"gcr.io/kubernetes-helm/tiller:v2.9.1\": rpc error: code = Unknown desc = Get https://gcr.io/v1/_ping: dial tcp 64.233.189.82:443: i/o timeout Normal BackOff 11m (x3221 over 14h) kubelet, k8s-node1 Back-off pulling image \"gcr.io/kubernetes-helm/tiller:v2.9.1\" Warning Failed 6m (x3237 over 14h) kubelet, k8s-node1 Error: ImagePullBackOff Warning Failed 1m (x15 over 14h) kubelet, k8s-node1 Failed to pull image \"gcr.io/kubernetes-helm/tiller:v2.9.1\": rpc error: code = Unknown desc = Get https://gcr.io/v1/_ping: dial tcp 64.233.188.82:443: i/o timeout 把这个没拉取到镜像想办法下载到这台机器上。当我们看到状态为 Running 说明 tiller 已经成功运行了: $ kubectl get pods -n kube-system | grep tiller tiller-deploy-dccdb6fd9-2df4r 1/1 Running 1 41d 默认安装的 tiller 权限很小，我们执行下面的脚本给它加最大权限，这样方便我们可以用 helm 部署应用到任意 namespace 下: kubectl create serviceaccount --namespace=kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace=kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' 更多参考官方文档: https://helm.sh/docs/using_helm/#quickstart-guide 安装 Helm V3 在 https://github.com/helm/helm/releases 找到对应系统的二进制包下载，比如下载 v3.0.0-beta.3 的 linux amd64 版: $ wget https://get.helm.sh/helm-v3.0.0-beta.3-linux-amd64.tar.gz 解压并移动到 PATH 下面: $ tar -zxvf helm-v3.0.0-beta.3-linux-amd64.tar.gz linux-amd64/ linux-amd64/LICENSE linux-amd64/helm linux-amd64/README.md $ cd linux-amd64/ $ ls LICENSE README.md helm $ mv helm /usr/local/bin/helm3 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"configuration-management/helm/upgrade-helm-v2-to-v3.html":{"url":"configuration-management/helm/upgrade-helm-v2-to-v3.html","title":"Helm V2 迁移到 V3","keywords":"","body":"Helm V2 迁移到 V3 Helm V3 与 V2 版本架构变化较大，数据迁移比较麻烦，官方提供了一个名为 helm-2to3 的插件来简化迁移工作，本文将介绍如何利用此插件迁移 Helm V2 到 V3 版本。这里前提是 Helm V3 已安装，安装方法请参考 这里。 安装 2to3 插件 一键安装: $ helm3 plugin install https://github.com/helm/helm-2to3 Downloading and installing helm-2to3 v0.1.1 ... https://github.com/helm/helm-2to3/releases/download/v0.1.1/helm-2to3_0.1.1_linux_amd64.tar.gz Installed plugin: 2to3 检查插件是否安装成功: $ helm3 plugin list NAME VERSION DESCRIPTION 2to3 0.1.1 migrate Helm v2 configuration and releases in-place to Helm v3 迁移 Helm V2 配置 $ helm3 2to3 move config [Helm 2] Home directory: /root/.helm [Helm 3] Config directory: /root/.config/helm [Helm 3] Data directory: /root/.local/share/helm [Helm 3] Create config folder \"/root/.config/helm\" . [Helm 3] Config folder \"/root/.config/helm\" created. [Helm 2] repositories file \"/root/.helm/repository/repositories.yaml\" will copy to [Helm 3] config folder \"/root/.config/helm/repositories.yaml\" . [Helm 2] repositories file \"/root/.helm/repository/repositories.yaml\" copied successfully to [Helm 3] config folder \"/root/.config/helm/repositories.yaml\" . [Helm 3] Create data folder \"/root/.local/share/helm\" . [Helm 3] data folder \"/root/.local/share/helm\" created. [Helm 2] plugins \"/root/.helm/plugins\" will copy to [Helm 3] data folder \"/root/.local/share/helm/plugins\" . [Helm 2] plugins \"/root/.helm/plugins\" copied successfully to [Helm 3] data folder \"/root/.local/share/helm/plugins\" . [Helm 2] starters \"/root/.helm/starters\" will copy to [Helm 3] data folder \"/root/.local/share/helm/starters\" . [Helm 2] starters \"/root/.helm/starters\" copied successfully to [Helm 3] data folder \"/root/.local/share/helm/starters\" . 上面的操作主要是迁移: Chart 仓库 Helm 插件 Chart starters 检查下 repo 和 plugin: $ helm3 repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts $ $ $ helm3 plugin list NAME VERSION DESCRIPTION 2to3 0.1.1 migrate Helm v2 configuration and releases in-place to Helm v3 push 0.1.1 Push chart package to TencentHub 迁移 Heml V2 Release 已经用 Helm V2 部署的应用也可以使用 2to3 的 convert 子命令迁移到 V3，先看下有哪些选项: $ helm3 2to3 convert --help migrate Helm v2 release in-place to Helm v3 Usage: 2to3 convert [flags] RELEASE Flags: --delete-v2-releases v2 releases are deleted after migration. By default, the v2 releases are retained --dry-run simulate a convert -h, --help help for convert -l, --label string label to select tiller resources by (default \"OWNER=TILLER\") -s, --release-storage string v2 release storage type/object. It can be 'secrets' or 'configmaps'. This is only used with the 'tiller-out-cluster' flag (default \"secrets\") -t, --tiller-ns string namespace of Tiller (default \"kube-system\") --tiller-out-cluster when Tiller is not running in the cluster e.g. Tillerless --tiller-out-cluster: 如果你的 Helm V2 是 tiller 在集群外面 (tillerless) 的安装方式，请带上这个参数 --dry-run: 模拟迁移但不做真实迁移操作，建议每次迁移都先带上这个参数测试下效果，没问题的话再去掉这个参数做真实迁移 --tiller-ns: 通常 tiller 如果部署在集群中，并且不在 kube-system 命名空间才指定 看下目前有哪些 helm v2 的 release: $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE redis 1 Mon Sep 16 14:46:58 2019 DEPLOYED redis-9.1.3 5.0.5 default 选一个用 --dry-run 试下效果: $ helm3 2to3 convert redis --dry-run NOTE: This is in dry-run mode, the following actions will not be executed. Run without --dry-run to take the actions described below: Release \"redis\" will be converted from Helm 2 to Helm 3. [Helm 3] Release \"redis\" will be created. [Helm 3] ReleaseVersion \"redis.v1\" will be created. 没有报错，去掉 --dry-run 执行迁移: $ helm3 2to3 convert redis Release \"redis\" will be converted from Helm 2 to Helm 3. [Helm 3] Release \"redis\" will be created. [Helm 3] ReleaseVersion \"redis.v1\" will be created. [Helm 3] ReleaseVersion \"redis.v1\" created. [Helm 3] Release \"redis\" created. Release \"redis\" was converted successfully from Helm 2 to Helm 3. Note: the v2 releases still remain and should be removed to avoid conflicts with the migrated v3 releases. 检查迁移结果: $ helm ls NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE redis 1 Mon Sep 16 14:46:58 2019 DEPLOYED redis-9.1.3 5.0.5 default $ $ $ helm3 ls -a NAME NAMESPACE REVISION UPDATED STATUS CHART redis default 1 2019-09-16 06:46:58.541391356 +0000 UTC deployed redis-9.1.3 helm 3 的 release 区分了命名空间，带上 -a 参数展示所有命名空间的 release 参考资料 How to migrate from Helm v2 to Helm v3: https://helm.sh/blog/migrate-from-helm-v2-to-helm-v3/ © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"security/cert-manager/install-cert-manger.html":{"url":"security/cert-manager/install-cert-manger.html","title":"安装 cert-manager","keywords":"","body":"安装 cert-manager 参考官方文档: https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html 介绍几种安装方式，不管是用哪种我们都先规划一下使用哪个命名空间，推荐使用 cert-manger 命名空间，如果使用其它的命名空间需要做些更改，会稍微有点麻烦，先创建好命名空间: kubectl create namespace cert-manager cert-manager 部署时会生成 ValidatingWebhookConfiguration 来注册 [ValidatingAdmissionWebhook])(ValidatingAdmissionWebhook) 来实现 CRD 校验，而ValidatingWebhookConfiguration 里需要写入 cert-manager 自身校验服务端的证书信息，就需要在自己命名空间创建 ClusterIssuer 和 Certificate 来自动创建证书，创建这些 CRD 资源又会被校验服务端校验，但校验服务端证书还没有创建所以校验请求无法发送到校验服务端，这就是一个鸡生蛋还是蛋生鸡的问题了，所以我们需要关闭 cert-manager 所在命名空间的 CRD 校验，通过打 label 来实现: kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true 使用原生 yaml 资源安装 直接执行 kubectl apply 来安装: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.0/cert-manager.yaml 使用 kubectl v1.12 及其以下的版本需要加上 --validate=false，否则会报错(参考 issue #69590): error: error validating \"https://github.com/jetstack/cert-manager/releases/download/v0.10.0/cert-manager.yaml\": error validating data: ValidationError(MutatingWebhookConfiguration.webhooks[0].clientConfig): missing required field \"caBundle\" in io.k8s.api.admissionregistration.v1beta1.WebhookClientConfig; if you choose to ignore these errors, turn validation off with --validate=false 使用 Helm 安装 安装 CRD: kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml 添加 repo: helm repo add jetstack https://charts.jetstack.io helm repo update 执行安装: helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.10.0 \\ jetstack/cert-manager 校验是否安装成功 检查 cert-manager 相关的 pod 是否启动成功: $ kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c6866597-zw7kh 1/1 Running 0 2m cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m 安装有问题可以参考官方的 troubleshooting guide © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"security/cert-manager/autogenerate-certificate-with-cert-manager.html":{"url":"security/cert-manager/autogenerate-certificate-with-cert-manager.html","title":"使用 cert-manager 自动生成证书","keywords":"","body":"使用 cert-manager 自动生成证书 确保 cert-manager 已安装，参考 安装 cert-manager 利用 Let’s Encrypt 生成免费证书 免费证书颁发原理 Let’s Encrypt 利用 ACME 协议来校验域名是否真的属于你，校验成功后就可以自动颁发免费证书，证书有效期只有 90 天，在到期前需要再校验一次来实现续期，幸运的是 cert-manager 可以自动续期，这样就可以使用永久免费的证书了。主流的两种校验方式是 HTTP-01 和 DNS-01，下面简单介绍下校验原理: HTTP-01 校验原理 HTTP-01 的校验原理是给你域名指向的 HTTP 服务增加一个临时 location ，Let’s Encrypt 会发送 http 请求到 http:///.well-known/acme-challenge/，YOUR_DOMAIN 就是被校验的域名，TOKEN 是 ACME 协议的客户端负责放置的文件，在这里 ACME 客户端就是 cert-manager，它通过修改 Ingress 规则来增加这个临时校验路径并指向提供 TOKEN 的服务。Let’s Encrypt 会对比 TOKEN 是否符合预期，校验成功后就会颁发证书。此方法仅适用于给使用 Ingress 暴露流量的服务颁发证书，并且不支持泛域名证书。 DNS-01 校验原理 DNS-01 的校验原理是利用 DNS 提供商的 API Key 拿到你的 DNS 控制权限， 在 Let’s Encrypt 为 ACME 客户端提供令牌后，ACME 客户端 (cert-manager) 将创建从该令牌和您的帐户密钥派生的 TXT 记录，并将该记录放在 _acme-challenge.。 然后 Let’s Encrypt 将向 DNS 系统查询该记录，如果找到匹配项，就可以颁发证书。此方法不需要你的服务使用 Ingress，并且支持泛域名证书。 创建颁发机构 (ClusterIssuer/Issuer) 我们需要先创建一个签发机构，cert-manager 给我们提供了 Issuer 和 ClusterIssuer 这两种用于创建签发机构的自定义资源对象，他们唯一区别就是 Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书，这里以 ClusterIssuer 为例，HTTP-01 和 DNS-01 校验都支持，假设域名是用 cloudflare 管理的，先登录 cloudflare 拿到 API Key，然后创建一个 Secret: kubectl -n cert-manager create secret generic cloudflare-apikey --from-literal=apikey=213807bd0fb1ca59bba24a58eac90492e6287 由于 ClusterIssuer 是 NonNamespaced 类型的资源，不在任何命名空间，它需要引用 Secret，而 Secret 必须存在某个命名空间下，所以就规定 ClusterIssuer 引用的 Secret 要与 cert-manager 在同一个命名空间下。 下面来创建 ClusterIssuer: - http01: # Enable the HTTP-01 challenge provider ingress: {} cat metadata.name: 是我们创建的签发机构的名称，后面我们创建证书的时候会引用它 acme.email: 是你自己的邮箱，证书快过期的时候会有邮件提醒，不过 cert-manager 会利用 acme 协议自动给我们重新颁发证书来续期 acme.server: 是 acme 协议的服务端，我们这里用 Let’s Encrypt，这个地址就写死成这样就行 acme.privateKeySecretRef 指示此签发机构的私钥将要存储到哪个 Secret 中，在 cert-manager 所在命名空间 acme.http01: 这里指示签发机构支持使用 HTTP-01 的方式进行 acme 协议 acme.dns01: 配置 DNS-01 校验方式所需的参数，最重要的是 API Key (引用提前创建好的 Secret)，不同 DNS 提供商配置不一样，具体参考官方API文档 更多字段参考 API 文档: https://docs.cert-manager.io/en/latest/reference/api-docs/index.html#clusterissuer-v1alpha1 创建证书 (Certificate) 有了签发机构，接下来我们就可以生成免费证书了，cert-manager 给我们提供了 Certificate 这个用于生成证书的自定义资源对象，它必须局限在某一个 namespace 下，证书最终会在这个 namespace 下以 Secret 的资源对象存储，假如我想在 dashboard 这个 namespace 下生成免费证书（这个 namespace 已存在)，创建一个 Certificate 资源来为我们自动生成证书，以 kubernetes dashboard 为例，分别示范下 HTTP-01 和 DNS-01 两种校验方式生成证书。 HTTP-01 方式 提前为服务创建好 Ingress: cat 检查 Ingress: $ kubectl -n kube-system get ingress NAME HOSTS ADDRESS PORTS AGE dashboard dashboard.imroc.io 150.109.28.133 80 19s 配置 DNS，将域名指向 Ingress 的 IP，然后创建 Certificate: cat secretName: 指示证书最终存到哪个 Secret 中 issuerRef.kind: 值为 ClusterIssuer 说明签发机构不在本 namespace 下，而是在全局 issuerRef.name: 我们创建的签发机构的名称 (ClusterIssuer.metadata.name) dnsNames: 指示该证书的可以用于哪些域名 acme.config.http01.ingress: 使用 HTTP-01 方式校验该域名和机器时，指定后端服务所在 ingress 名称，cert-manager 会尝试修改该 ingress 规则，增加临时路径进行 ACME 协议的 HTTP-01 方式校验。如果你使用的 ingress controller 是所有 ingress 都用同一个入口 IP，比如 nginx ingress，这时你可以不用提前创建 ingress，只需要指定 ingressClass 就可以，cert-manager 会自动创建 ingress 包含 HTTP-01 临时校验路径，并指定 kubernetes.io/ingress.class 这个 annotation，然后你的 ingress controller 会自动根据该 ingress 更新转发规则，从而实现 ACME 协议的 HTTP-01 方式校验。 acme.config.http01.domains: 指示该证书的可以用于哪些域名 更多字段参考 API 文档: https://docs.cert-manager.io/en/latest/reference/api-docs/index.html#certificate-v1alpha1 DNS-01 方式 cat 检查结果 创建完成等待一段时间，校验成功颁发证书后会将证书信息写入 Certificate 所在命名空间的 secretName 指定的 Secret 中，其它应用需要证书就可以直接挂载该 Secret 了。 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Generated 15s cert-manager Generated new private key Normal GenerateSelfSigned 15s cert-manager Generated temporary self signed certificate Normal OrderCreated 15s cert-manager Created Order resource \"dashboard-imroc-io-780134401\" Normal OrderComplete 9s cert-manager Order \"dashboard-imroc-io-780134401\" completed successfully Normal CertIssued 9s cert-manager Certificate issued successfully 看下我们的证书是否成功生成: kubectl -n dashboard get secret kubernetes-dashboard-certs -o yaml apiVersion: v1 data: ca.crt: null tls.crt: LS0***0tLQo= tls.key: LS0***0tCg== kind: Secret metadata: annotations: certmanager.k8s.io/alt-names: dashboard.imroc.io certmanager.k8s.io/certificate-name: dashboard-imroc-io certmanager.k8s.io/common-name: dashboard.imroc.io certmanager.k8s.io/ip-sans: \"\" certmanager.k8s.io/issuer-kind: ClusterIssuer certmanager.k8s.io/issuer-name: letsencrypt-prod creationTimestamp: 2019-09-19T13:53:55Z labels: certmanager.k8s.io/certificate-name: dashboard-imroc-io name: kubernetes-dashboard-certs namespace: dashboard resourceVersion: \"5689447213\" selfLink: /api/v1/namespaces/dashboard/secrets/kubernetes-dashboard-certs uid: ebfc4aec-dae4-11e9-89f7-be8690a7fdcf type: kubernetes.io/tls tls.crt 就是颁发的证书 tls.key 是证书密钥 将 secret 挂载到需要证书的应用，通常应用也要配置下证书路径。 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "},"storage/install-elasticsearch-and-kibana-with-elastic-oparator.html":{"url":"storage/install-elasticsearch-and-kibana-with-elastic-oparator.html","title":"使用 elastic-oparator 部署 Elasticsearch 和 Kibana","keywords":"","body":"使用 elastic-oparator 部署 Elasticsearch 和 Kibana 参考官方文档: https://www.elastic.co/cn/elasticsearch-kubernetes https://www.elastic.co/cn/blog/introducing-elastic-cloud-on-kubernetes-the-elasticsearch-operator-and-beyond 安装 elastic-operator 一键安装: kubectl apply -f https://download.elastic.co/downloads/eck/0.9.0/all-in-one.yaml 部署 Elasticsearch 准备一个命名空间用来部署 elasticsearch，这里我们使用 monitoring 命名空间: kubectl create ns monitoring 创建 CRD 资源部署 Elasticsearch，最简单的部署: cat 多节点部署高可用 elasticsearch 集群: cat metadata.name 是 elasticsearch 集群的名称 nodeCount 大于 1 (多副本) 并且加了 pod 反亲和性 (避免调度到同一个节点) 可避免单点故障，保证高可用 node.master 为 true 表示是 master 节点 可根据需求调整 nodeCount (副本数量) 和 storage (数据磁盘容量) 反亲和性的 labelSelector.matchExpressions.values 中写 elasticsearch 集群名称，更改集群名称时记得这里要也改下 强制开启 ssl 不允许关闭: https://github.com/elastic/cloud-on-k8s/blob/576f07faaff4393f9fb247e58b87517f99b08ebd///pkg/controller/elasticsearch/settings/fields.go#L51 查看部署状态: $ kubectl -n monitoring get es NAME HEALTH NODES VERSION PHASE AGE es green 3 7.2.0 Operational 3m $ $ kubectl -n monitoring get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE es-es-c7pwnt5kz8 1/1 Running 0 4m3s 172.16.4.6 10.0.0.24 es-es-qpk7kkpdxh 1/1 Running 0 4m3s 172.16.5.6 10.0.0.48 es-es-vl56nv78hd 1/1 Running 0 4m3s 172.16.3.9 10.0.0.32 $ $ kubectl -n monitoring get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE es-es-http ClusterIP 172.16.15.74 9200/TCP 7m3s elasticsearch 的默认用户名是 elastic，获取密码: $ kubectl -n monitoring get secret es-es-elastic-user -o jsonpath='{.data.elastic}' | base64 -d rhd6jdw9brbj69d49k46px9j 后续连接 elasticsearch 时就用这对用户名密码: username: elastic password: rhd6jdw9brbj69d49k46px9j 部署 Kibana 还可以再部署一个 Kibana 集群作为 UI: config: server.ssl.enabled: false http.tls.selfSignedCertificate.disabled: true cat nodeCount 大于 1 (多副本) 并且加了 pod 反亲和性 (避免调度到同一个节点) 可避免单点故障，保证高可用 反亲和性的 labelSelector.matchExpressions.values 中写 kibana 集群名称，更改集群名称时记得这里要也改下 elasticsearchRef 引用已经部署的 elasticsearch 集群，name 和 namespace 分别填部署的 elasticsearch 集群名称和命名空间 查看部署状态: $ kubectl -n monitoring get kb NAME HEALTH NODES VERSION AGE kibana green 2 7.2.0 3m $ $ kubectl -n monitoring get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE kibana-kb-58dc8994bf-224bl 1/1 Running 0 93s 172.16.0.92 10.0.0.3 kibana-kb-58dc8994bf-nchqt 1/1 Running 0 93s 172.16.3.10 10.0.0.32 $ $ kubectl -n monitoring get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kibana-kb-http ClusterIP 172.16.8.71 5601/TCP 4m35s 还需要为 Kibana 暴露一个外部地址好让我们能从从浏览器访问，可以创建 Service 或 Ingress 来实现。 默认也会为 Kibana 创建 ClusterIP 类型的 Service，可以在 Kibana 的 CRD spec 里加 service 来自定义 service type 为 LoadBalancer 实现对外暴露，但我不建议这么做，因为一旦删除 CRD 对象，service 也会被删除，在云上通常意味着对应的负载均衡器也被自动删除，IP 地址就会被回收，下次再创建的时候 IP 地址就变了，所以推荐对外暴露方式使用单独的 Service 或 Ingress 来维护 创建 Service 先看下当前 kibana 的 service: $ kubectl -n monitoring get svc -o yaml kibana-kb-http apiVersion: v1 kind: Service metadata: creationTimestamp: 2019-09-17T09:20:04Z labels: common.k8s.elastic.co/type: kibana kibana.k8s.elastic.co/name: kibana name: kibana-kb-http namespace: monitoring ownerReferences: - apiVersion: kibana.k8s.elastic.co/v1alpha1 blockOwnerDeletion: true controller: true kind: Kibana name: kibana uid: 54fd304b-d92c-11e9-89f7-be8690a7fdcf resourceVersion: \"5668802758\" selfLink: /api/v1/namespaces/monitoring/services/kibana-kb-http uid: 55a1198f-d92c-11e9-89f7-be8690a7fdcf spec: clusterIP: 172.16.8.71 ports: - port: 5601 protocol: TCP targetPort: 5601 selector: common.k8s.elastic.co/type: kibana kibana.k8s.elastic.co/name: kibana sessionAffinity: None type: ClusterIP status: loadBalancer: {} 仅保留端口和 selector 的配置，如果集群支持 LoadBanlancer 类型的 service，可以修改 service 的 type 为 LoadBalancer: cat 拿到负载均衡器的 IP 地址: $ kubectl -n monitoring get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kibana LoadBalancer 172.16.10.71 150.109.27.60 443:32749/TCP 47s kibana-kb-http ClusterIP 172.16.15.39 5601/TCP 118s 在浏览器访问: https://150.109.27.60:443 输入之前部署 elasticsearch 的用户名密码进行登录 © roc all right reserved，powered by GitbookUpdated at 2019-09-20 13:47:22 "}}